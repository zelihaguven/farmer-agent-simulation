{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8fc7b3a",
   "metadata": {},
   "source": [
    "#  Ã‡iftÃ§i Ajan Projesi\n",
    "\n",
    "Bu proje, **model-tabanlÄ± refleks ajanlarÄ±n** kÄ±smi gÃ¶zlemlenebilir bir Ã§iftlik ortamÄ±nda nasÄ±l farklÄ± stratejilerle karar verdiklerini simÃ¼le eder.  \n",
    "Ajanlar, **yumurta (Y)** ve **sÃ¼t (S)** toplarken **bataklÄ±k (B)** ve **ayÄ± (A)** gibi tehlikelerden kaÃ§Ä±narak **maksimum performans puanÄ±** elde etmeyi hedefler.\n",
    "\n",
    "Proje kapsamÄ±nda Ã¼Ã§ farklÄ± ajan geliÅŸtirilmiÅŸtir:\n",
    "\n",
    "| Ajan | DavranÄ±ÅŸ Tipi | KÄ±saca |\n",
    "|------|----------------|--------|\n",
    "| **Agent1** | Temel Model-TabanlÄ± Refleks Ajan | Hedef odaklÄ±, planlÄ± hareket eder |\n",
    "| **Agent2** | GeliÅŸmiÅŸ Model-TabanlÄ± Refleks Ajan | Tehlikeleri belleÄŸinde tutar, riskten kaÃ§Ä±nÄ±r |\n",
    "| **Agent3** | Rastgele Model-TabanlÄ± Refleks Ajan | Model yapÄ±sÄ±nÄ± korur fakat rastgele kararlar verir |\n",
    "\n",
    "---\n",
    "\n",
    "##  AmaÃ§ (PEAS TanÄ±mÄ±)\n",
    "\n",
    "| BileÅŸen | AÃ§Ä±klama |\n",
    "|----------|-----------|\n",
    "| **Performance (P)** | Yumurta toplamak **+2**, sÃ¼t toplamak **+5**, bataklÄ±ÄŸa girmek **-10**, ayÄ±ya girmek **-100** puan etkiler. |\n",
    "| **Environment (E)** | 10Ã—10 Ä±zgara harita. Kaynaklar (S, Y) ve tehlikeler (A, B) sabit konumludur. |\n",
    "| **Actuators (A)** | `FORWARD`, `LEFT`, `RIGHT`, `LOAD` hareketleri. |\n",
    "| **Sensors (S)** | Ajan, bulunduÄŸu hÃ¼cre ve dÃ¶rt komÅŸunun durumunu algÄ±layabilir. |\n",
    "\n",
    "---\n",
    "\n",
    "##  Ortam Ã–zellikleri\n",
    "\n",
    "| Ã–zellik | DeÄŸer |\n",
    "|----------|--------|\n",
    "| GÃ¶zlemlenebilirlik | **KÄ±smi** â€“ yalnÄ±zca mevcut konum ve komÅŸu hÃ¼creler algÄ±lanÄ±r |\n",
    "| Deterministik | **Evet** â€“ aynÄ± hareket her zaman aynÄ± sonucu Ã¼retir |\n",
    "| Dinamiklik | **Statik** â€“ Ã§evre zamanla deÄŸiÅŸmez |\n",
    "| Ajan SayÄ±sÄ± | **Tek ajan** |\n",
    "| Episodiklik | **Sekansiyel** â€“ geÃ§miÅŸ kararlar sonraki adÄ±mlarÄ± etkiler |\n",
    "\n",
    "---\n",
    "\n",
    "##  Ajan TÃ¼rleri ve DavranÄ±ÅŸ FarklarÄ±\n",
    "\n",
    "###  **Agent1 â€“ Temel Model-TabanlÄ± Refleks Ajan**\n",
    "\n",
    "- Ã‡evreyi algÄ±lar ve **gÃ¶zlem geÃ§miÅŸini belleÄŸinde tutar.**\n",
    "- Bilinen kaynaklara yÃ¶nelir, yoksa rastgele keÅŸif yapar.\n",
    "- Tehlike bilgisini dikkate almaz.\n",
    "\n",
    "**Karar MekanizmasÄ±:**\n",
    "1. BulunduÄŸu hÃ¼crede kaynak varsa â†’ **LOAD**  \n",
    "2. HafÄ±zasÄ±nda bilinen kaynak varsa â†’ **YÃ¶ne dÃ¶n ve ilerle**  \n",
    "3. Bilgi yoksa â†’ **Rastgele keÅŸfet (Ã¶ncelikli ileri)**  \n",
    "\n",
    "---\n",
    "\n",
    "###  **Agent2 â€“ Risk OdaklÄ± Model-TabanlÄ± Refleks Ajan**\n",
    "\n",
    "- Agent1â€™in tÃ¼m Ã¶zelliklerine ek olarak:\n",
    "  - **Tehlikeli hÃ¼creleri (A, B)** hafÄ±zaya kaydeder.\n",
    "  - Riskli yÃ¶nleri tercih etmez.\n",
    "- Daha gÃ¼venli bir rota belirler, verimlilik artar.\n",
    "\n",
    "**Ek Farklar:**\n",
    "- Tehlikeden kaÃ§Ä±nmak iÃ§in hafÄ±zadaki bilgiye dayanÄ±r.  \n",
    "- Ã–ncelikli hedef: **Kaynak toplamak + riskten kaÃ§Ä±nmak.**\n",
    "\n",
    "---\n",
    "\n",
    "###  **Agent3 â€“ Rastgele Hareketli Ajan**\n",
    "\n",
    "- Model tabanlÄ± yapÄ±ya sahip olsa da **bellek veya karar mantÄ±ÄŸÄ± yoktur.**\n",
    "- Her adÄ±mda rastgele yÃ¶n ve hareket seÃ§er.\n",
    "- **ZekÃ¢ seviyesi dÃ¼ÅŸÃ¼ktÃ¼r**, ancak â€œdavranÄ±ÅŸ testiâ€ iÃ§in kullanÄ±lÄ±r.\n",
    "\n",
    "**AmaÃ§:**  \n",
    "Rastgele hareket eden bir ajanÄ±n performansÄ±nÄ± Ã¶lÃ§erek, refleks ve bellekli ajanlarla karÅŸÄ±laÅŸtÄ±rmak.\n",
    "\n",
    "---\n",
    "\n",
    "##  KarÅŸÄ±laÅŸtÄ±rmalÄ± Ã–zellik Tablosu\n",
    "\n",
    "| Ã–zellik | Agent1 | Agent2 | Agent3 |\n",
    "|----------|---------|---------|---------|\n",
    "| Model TabanlÄ±lÄ±k | âœ… | âœ… | âœ… |\n",
    "| Bellek / HafÄ±za | ğŸŸ¡ (Kaynak) | ğŸŸ¢ (Kaynak + Tehlike) | âŒ |\n",
    "| Karar MekanizmasÄ± | Hedef odaklÄ± | Hedef + risk analizi | Rastgele |\n",
    "| Ã–ÄŸrenme YeteneÄŸi | âŒ | âŒ | âŒ |\n",
    "| ZekÃ¢ Seviyesi | Orta | YÃ¼ksek | DÃ¼ÅŸÃ¼k |\n",
    "| Performans Beklentisi | Orta | YÃ¼ksek | DÃ¼ÅŸÃ¼k |\n",
    "| Rastgelelik | DÃ¼ÅŸÃ¼k | DÃ¼ÅŸÃ¼k | YÃ¼ksek |\n",
    "\n",
    "---\n",
    "\n",
    "##  Kod YapÄ±sÄ±\n",
    "\n",
    "| Dosya / SÄ±nÄ±f | AÃ§Ä±klama |\n",
    "|----------------|-----------|\n",
    "| `XYEnvironment` | 10Ã—10 ortamÄ±n haritasÄ±nÄ±, sÄ±nÄ±rlarÄ±, kaynaklarÄ± ve tehlikeleri yÃ¶netir. |\n",
    "| `Agent1` / `Agent2` / `Agent3` | FarklÄ± stratejilerle karar veren ajan sÄ±nÄ±flarÄ±. |\n",
    "| `create_grid()` | HaritayÄ± oluÅŸturur; kaynak ve tehlike yerleÅŸimlerini belirler. |\n",
    "| `run_episode()` | SimÃ¼lasyonu baÅŸlatÄ±r, ajan hareketlerini Ã§alÄ±ÅŸtÄ±rÄ±r ve toplam puanÄ± dÃ¶ndÃ¼rÃ¼r. |\n",
    "| `display_grid()` | SimÃ¼lasyonun gÃ¶rsel Ã§Ä±ktÄ±sÄ±nÄ± (Ä±zgara halinde) ekrana yazdÄ±rÄ±r. |\n",
    "\n",
    "---\n",
    "\n",
    "##  Performans DeÄŸerlendirmesi\n",
    "\n",
    "Ajanlar, aynÄ± harita Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±larak **ortalama performans**, **toplanan kaynak sayÄ±sÄ±** ve **maruz kalÄ±nan tehlike sayÄ±sÄ±** aÃ§Ä±sÄ±ndan kÄ±yaslanÄ±r.\n",
    "\n",
    "| Metrik | Agent1 | Agent2 | Agent3 |\n",
    "|--------|---------|---------|---------|\n",
    "| Toplanan Kaynak | Orta | YÃ¼ksek | DÃ¼ÅŸÃ¼k |\n",
    "| Tehlike Ã‡arpÄ±ÅŸmasÄ± | Orta | DÃ¼ÅŸÃ¼k | YÃ¼ksek |\n",
    "| Ortalama Skor | Orta | YÃ¼ksek | DÃ¼ÅŸÃ¼k |\n",
    "\n",
    "---\n",
    "\n",
    "##  SonuÃ§\n",
    "\n",
    "Bu Ã§alÄ±ÅŸma, **model tabanlÄ± refleks ajanlarÄ±n bellek, karar verme ve rastgelelik seviyelerine gÃ¶re performans farklarÄ±nÄ±** gÃ¶stermektedir.  \n",
    "SonuÃ§ olarak:\n",
    "\n",
    "- **Agent2**, dengeli stratejisiyle en baÅŸarÄ±lÄ± ajandÄ±r.  \n",
    "- **Agent1**, sÄ±nÄ±rlÄ± hafÄ±zasÄ±yla orta dÃ¼zeyde performans sergiler.  \n",
    "- **Agent3**, karar zekÃ¢sÄ± olmadan rastgele hareket ettiÄŸi iÃ§in dÃ¼ÅŸÃ¼k performans gÃ¶sterir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e33423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimÃ¼lasyon baÅŸlatÄ±lÄ±yor...\n",
      "\n",
      "Deneme 1: -93 puan\n",
      "Deneme 2: 7 puan\n",
      "Deneme 3: 7 puan\n",
      "Deneme 4: -93 puan\n",
      "Deneme 5: 7 puan\n",
      "\n",
      "========================================\n",
      "Ortalama Performans: -33.00 puan\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# AGENT1 - Temel Model-TabanlÄ± Refleks Ajan\n",
    "\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "import random\n",
    "import statistics\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "class Direction(Enum):\n",
    "    \"\"\"YÃ¶n tanÄ±mlarÄ±\"\"\"\n",
    "    NORTH = 0\n",
    "    EAST = 1\n",
    "    SOUTH = 2\n",
    "    WEST = 3\n",
    "\n",
    "# Her yÃ¶n iÃ§in hareket deltalarÄ±\n",
    "move_delta = {\n",
    "    Direction.NORTH: (-1, 0),\n",
    "    Direction.SOUTH: (1, 0),\n",
    "    Direction.EAST: (0, 1),\n",
    "    Direction.WEST: (0, -1)\n",
    "}\n",
    "\n",
    "class XYEnvironment:\n",
    "    \"\"\"\n",
    "    10x10 grid ortamÄ±.\n",
    "    - 'E': Engel (geÃ§ilemez)\n",
    "    - 'Y': Yumurta (+2 puan)\n",
    "    - 'S': SÃ¼t (+5 puan)\n",
    "    - 'B': BataklÄ±k (-10 puan)\n",
    "    - 'A': AyÄ± (-100 puan)\n",
    "    - 'C': BaÅŸlangÄ±Ã§ pozisyonu\n",
    "    - ' ': BoÅŸ alan\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid, start):\n",
    "        self.grid = [row[:] for row in grid]  # Deep copy\n",
    "        self.start = start\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "    \n",
    "    def in_bounds(self, pos):\n",
    "        \"\"\"Pozisyon sÄ±nÄ±rlar iÃ§inde mi?\"\"\"\n",
    "        r, c = pos\n",
    "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
    "    \n",
    "    def is_obstacle(self, pos):\n",
    "        \"\"\"Pozisyon engel mi?\"\"\"\n",
    "        r, c = pos\n",
    "        return self.grid[r][c] == \"E\"\n",
    "    \n",
    "    def percept(self, pos):\n",
    "        \"\"\"\n",
    "        KÄ±smi gÃ¶zlem: Bulunulan hÃ¼cre + 4 komÅŸu hÃ¼crenin iÃ§eriÄŸi\n",
    "        \"\"\"\n",
    "        r, c = pos\n",
    "        obs = {}\n",
    "        obs['current'] = self.grid[r][c]\n",
    "        \n",
    "        neighbours = {}\n",
    "        for d, delta in move_delta.items():\n",
    "            nr, nc = r + delta[0], c + delta[1]\n",
    "            if self.in_bounds((nr, nc)):\n",
    "                neighbours[d.name] = self.grid[nr][nc]\n",
    "            else:\n",
    "                neighbours[d.name] = None\n",
    "        obs['neighbours'] = neighbours\n",
    "        return obs\n",
    "    \n",
    "    def execute(self, agent, action):\n",
    "        \"\"\"\n",
    "        Aksiyonu uygula ve performans deÄŸiÅŸimini dÃ¶ndÃ¼r.\n",
    "        Aksiyonlar: 'FORWARD', 'LEFT', 'RIGHT', 'LOAD'\n",
    "        \"\"\"\n",
    "        perf_delta = 0\n",
    "        \n",
    "        if action == \"LEFT\":\n",
    "            # Sadece sola dÃ¶n\n",
    "            agent.orientation = Direction((agent.orientation.value - 1) % 4)\n",
    "            \n",
    "        elif action == \"RIGHT\":\n",
    "            # Sadece saÄŸa dÃ¶n\n",
    "            agent.orientation = Direction((agent.orientation.value + 1) % 4)\n",
    "            \n",
    "        elif action == \"FORWARD\":\n",
    "            # Ä°leri git\n",
    "            dr, dc = move_delta[agent.orientation]\n",
    "            newpos = (agent.pos[0] + dr, agent.pos[1] + dc)\n",
    "            \n",
    "            if not self.in_bounds(newpos) or self.is_obstacle(newpos):\n",
    "                # Engele veya sÄ±nÄ±r dÄ±ÅŸÄ±na gidilemez, yerinde kal\n",
    "                return perf_delta\n",
    "            \n",
    "            agent.pos = newpos\n",
    "            \n",
    "            # Yeni hÃ¼credeki tehlikeleri kontrol et\n",
    "            r, c = agent.pos\n",
    "            cell_content = self.grid[r][c]\n",
    "            if cell_content == \"B\":\n",
    "                perf_delta -= 10  # BataklÄ±k\n",
    "            elif cell_content == \"A\":\n",
    "                perf_delta -= 100  # AyÄ±\n",
    "                \n",
    "        elif action == \"LOAD\":\n",
    "            # Bulunulan hÃ¼credeki kaynaÄŸÄ± yÃ¼kle\n",
    "            r, c = agent.pos\n",
    "            content = self.grid[r][c]\n",
    "            \n",
    "            if content == \"Y\" and not agent.carry_egg:\n",
    "                agent.carry_egg = True\n",
    "                self.grid[r][c] = \" \"  # YumurtayÄ± haritadan kaldÄ±r\n",
    "                perf_delta += 2\n",
    "            elif content == \"S\" and not agent.carry_milk:\n",
    "                agent.carry_milk = True\n",
    "                self.grid[r][c] = \" \"  # SÃ¼tÃ¼ haritadan kaldÄ±r\n",
    "                perf_delta += 5\n",
    "            \n",
    "            # LOAD sonrasÄ± hÃ¼credeki tehlikeleri kontrol et\n",
    "            cell_content = self.grid[r][c]\n",
    "            if cell_content == \"B\":\n",
    "                perf_delta -= 10\n",
    "            elif cell_content == \"A\":\n",
    "                perf_delta -= 100\n",
    "        \n",
    "        return perf_delta\n",
    "\n",
    "\n",
    "class Agent:\n",
    "  \n",
    "    \n",
    "    def __init__(self, start, orientation=Direction.EAST):\n",
    "        self.pos = start\n",
    "        self.orientation = orientation\n",
    "        self.carry_egg = False\n",
    "        self.carry_milk = False\n",
    "        self.memory = {}  # GÃ¶zlemlenen hÃ¼crelerin hafÄ±zasÄ±\n",
    "    \n",
    "    def perceive(self, env):\n",
    "        \"\"\"Ortamdan algÄ±lama yap ve hafÄ±zaya kaydet\"\"\"\n",
    "        p = env.percept(self.pos)\n",
    "        \n",
    "        # Bulunulan hÃ¼creyi hafÄ±zaya kaydet\n",
    "        r, c = self.pos\n",
    "        self.memory[(r, c)] = p['current']\n",
    "        \n",
    "        # KomÅŸularÄ± hafÄ±zaya kaydet\n",
    "        for dname, content in p['neighbours'].items():\n",
    "            if content is not None:\n",
    "                dir_enum = Direction[dname]\n",
    "                dr, dc = move_delta[dir_enum]\n",
    "                nr, nc = r + dr, c + dc\n",
    "                self.memory[(nr, nc)] = content\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def program(self, percept, env):\n",
    "      \n",
    "        cur = percept['current']\n",
    "        \n",
    "        # 1. BulunduÄŸu yerde kaynak varsa yÃ¼kle\n",
    "        if cur == \"S\" and not self.carry_milk:\n",
    "            return \"LOAD\"\n",
    "        if cur == \"Y\" and not self.carry_egg:\n",
    "            return \"LOAD\"\n",
    "        \n",
    "        # 2. Bilinen kaynaklara yÃ¶nel\n",
    "        targets = []\n",
    "        for (pos, content) in self.memory.items():\n",
    "            if content == \"S\" and not self.carry_milk:\n",
    "                targets.append(pos)\n",
    "            elif content == \"Y\" and not self.carry_egg:\n",
    "                targets.append(pos)\n",
    "        \n",
    "        if targets:\n",
    "            # En yakÄ±n hedefi seÃ§ (Manhattan distance)\n",
    "            target = min(targets, key=lambda t: abs(t[0] - self.pos[0]) + abs(t[1] - self.pos[1]))\n",
    "            \n",
    "            # Hedefe doÄŸru yÃ¶nelim belirle\n",
    "            dr = target[0] - self.pos[0]\n",
    "            dc = target[1] - self.pos[1]\n",
    "            \n",
    "            # Ã–nce dikey, sonra yatay hareket tercih et\n",
    "            if dr < 0 and self.orientation != Direction.NORTH:\n",
    "                return self.turn_towards(Direction.NORTH)\n",
    "            elif dr > 0 and self.orientation != Direction.SOUTH:\n",
    "                return self.turn_towards(Direction.SOUTH)\n",
    "            elif dc < 0 and self.orientation != Direction.WEST:\n",
    "                return self.turn_towards(Direction.WEST)\n",
    "            elif dc > 0 and self.orientation != Direction.EAST:\n",
    "                return self.turn_towards(Direction.EAST)\n",
    "            else:\n",
    "                # DoÄŸru yÃ¶ne bakÄ±yoruz, ileri git\n",
    "                return \"FORWARD\"\n",
    "        \n",
    "        # 3. KeÅŸif: Ä°leri gidebiliyorsa %70 ileri git, yoksa dÃ¶n\n",
    "        dr, dc = move_delta[self.orientation]\n",
    "        forward_pos = (self.pos[0] + dr, self.pos[1] + dc)\n",
    "        \n",
    "        if env.in_bounds(forward_pos) and not env.is_obstacle(forward_pos):\n",
    "            if random.random() < 0.7:\n",
    "                return \"FORWARD\"\n",
    "        \n",
    "        # Rastgele dÃ¶n\n",
    "        return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "    \n",
    "    def turn_towards(self, desired_dir):\n",
    "        \"\"\"Ä°stenilen yÃ¶ne dÃ¶nmek iÃ§in LEFT veya RIGHT dÃ¶ndÃ¼r\"\"\"\n",
    "        diff = (desired_dir.value - self.orientation.value) % 4\n",
    "        \n",
    "        if diff == 0:\n",
    "            return \"FORWARD\"\n",
    "        elif diff == 1:\n",
    "            return \"RIGHT\"\n",
    "        elif diff == 3:\n",
    "            return \"LEFT\"\n",
    "        else:  # diff == 2 (ters yÃ¶n)\n",
    "            return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_grid():\n",
    "    \"\"\"10x10 grid haritasÄ± oluÅŸtur\"\"\"\n",
    "    # TÃ¼m kenarlar engel ile baÅŸla\n",
    "    grid = [[\"E\"] * 10 for _ in range(10)]\n",
    "    \n",
    "    # Ä°Ã§ kÄ±smÄ± boÅŸalt\n",
    "    for i in range(1, 9):\n",
    "        for j in range(1, 9):\n",
    "            grid[i][j] = \" \"\n",
    "    \n",
    "    # KaynaklarÄ± ve tehlikeleri yerleÅŸtir\n",
    "    placements = {\n",
    "        (1, 2): \"Y\", (2, 2): \"Y\", (2, 3): \"Y\",  # Yumurtalar\n",
    "        (1, 6): \"S\", (2, 6): \"S\", (5, 1): \"S\",  # SÃ¼tler\n",
    "        (4, 6): \"Y\", (4, 7): \"Y\",                # Daha fazla yumurta\n",
    "        (7, 5): \"B\", (6, 5): \"B\",                # BataklÄ±klar\n",
    "        (8, 2): \"A\"                               # AyÄ±\n",
    "    }\n",
    "    \n",
    "    for (r, c), value in placements.items():\n",
    "        grid[r][c] = value\n",
    "    \n",
    "    # BaÅŸlangÄ±Ã§ pozisyonu\n",
    "    start = (3, 1)\n",
    "    grid[start[0]][start[1]] = \"C\"\n",
    "    \n",
    "    return grid, start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_episode(grid, start, max_steps=50):\n",
    "    \"\"\"Bir epizodu Ã§alÄ±ÅŸtÄ±r ve toplam performansÄ± dÃ¶ndÃ¼r\"\"\"\n",
    "    env = XYEnvironment(grid, start)\n",
    "    agent = Agent(start, orientation=Direction.EAST)\n",
    "    total_perf = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        percept = agent.perceive(env)\n",
    "        action = agent.program(percept, env)\n",
    "        perf_delta = env.execute(agent, action)\n",
    "        total_perf += perf_delta\n",
    "    \n",
    "    return total_perf\n",
    "\n",
    "\n",
    "\n",
    "GRID, START = create_grid()\n",
    "performances = []\n",
    "\n",
    "print(\"SimÃ¼lasyon baÅŸlatÄ±lÄ±yor...\\n\")\n",
    "for i in range(5):\n",
    "    perf = run_episode(GRID, START, max_steps=50)\n",
    "    performances.append(perf)\n",
    "    print(f\"Deneme {i+1}: {perf} puan\")\n",
    "\n",
    "mean_perf = statistics.mean(performances)\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"Ortalama Performans: {mean_perf:.2f} puan\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd446462",
   "metadata": {},
   "source": [
    "## AGENT1 : \n",
    "\n",
    "###  AjanÄ±n Ã–zellikleri\n",
    "\n",
    "1. **Model-Based Reflex Agent (Model TabanlÄ± Refleks Ajan)**  \n",
    "   - Ajan, ortamdan aldÄ±ÄŸÄ± algÄ±larÄ± (percept) hafÄ±zasÄ±nda saklayarak model oluÅŸturur.  \n",
    "   - Ã–nceden gÃ¶zlemlenen hÃ¼crelerin iÃ§eriÄŸini (`self.memory`) kaydeder ve kararlarÄ±nda bu bilgiyi kullanÄ±r.\n",
    "\n",
    "2. **KÄ±smi GÃ¶zlem (Partial Observability) ile Ã‡alÄ±ÅŸma**  \n",
    "   - Ajan sadece bulunduÄŸu hÃ¼creyi ve dÃ¶rt komÅŸu hÃ¼creyi gÃ¶zlemleyebilir.  \n",
    "   - Bu, gerÃ§ekÃ§i bir kÄ±sÄ±t getirerek ajanÄ± belirsizlik altÄ±nda karar vermeye zorlar.\n",
    "\n",
    "3. **Hedef OdaklÄ± Karar Verme**  \n",
    "   - HafÄ±zasÄ±nda bulunan â€œSÃ¼t (S)â€ veya â€œYumurta (Y)â€ hÃ¼crelerine yÃ¶nelir.  \n",
    "   - EÄŸer bilinen bir kaynak yoksa, rastgele keÅŸif stratejisi uygular.\n",
    "\n",
    "4. **Eylem KÃ¼mesi**  \n",
    "   - `\"FORWARD\"`, `\"LEFT\"`, `\"RIGHT\"`, `\"LOAD\"` aksiyonlarÄ±nÄ± kullanÄ±r.  \n",
    "   - `\"LOAD\"` komutu, ajan bulunduÄŸu hÃ¼credeki kaynaÄŸÄ± topladÄ±ÄŸÄ±nda performans artÄ±ÅŸÄ± saÄŸlar.\n",
    "\n",
    "5. **Performans Ã–lÃ§Ã¼tÃ¼ (Performance Function)**  \n",
    "   - Pozitif Ã¶dÃ¼ller:  \n",
    "     - Yumurta toplama: **+2 puan**  \n",
    "     - SÃ¼t toplama: **+5 puan**  \n",
    "   - Negatif Ã¶dÃ¼ller:  \n",
    "     - BataklÄ±ÄŸa girme: **-10 puan**  \n",
    "     - AyÄ± hÃ¼cresine girme: **-100 puan**\n",
    "\n",
    "6. **Basit YÃ¶nelim Sistemi**  \n",
    "   - Ajan, bulunduÄŸu yÃ¶nÃ¼ `Direction` enum sÄ±nÄ±fÄ± ile temsil eder ve sola/saÄŸa dÃ¶nerek yÃ¶nÃ¼nÃ¼ deÄŸiÅŸtirir.  \n",
    "   - Hedef hÃ¼creye gÃ¶re yÃ¶nÃ¼nÃ¼ ayarlamak iÃ§in `turn_towards()` fonksiyonunu kullanÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "###  GeliÅŸtirilebilecek YÃ¶nler\n",
    "\n",
    "1. **Hedef Planlama ve Yol Bulma (Path Planning)**  \n",
    "   - Åu anda ajan sadece Manhattan mesafesine gÃ¶re en yakÄ±n hedefe yÃ¶neliyor.  \n",
    "   - Bunun yerine **BFS**, **A\\*** veya **Dijkstra** gibi algoritmalarla engelleri ve tehlikeleri dikkate alan bir yol planlama stratejisi eklenebilir.\n",
    "\n",
    "2. **Bellek GÃ¼ncelleme MekanizmasÄ±**  \n",
    "   - AjanÄ±n hafÄ±zasÄ± sadece gÃ¶zlemlenen hÃ¼creleri kaydediyor, ancak ortamda deÄŸiÅŸim (Ã¶rneÄŸin kaynaklarÄ±n toplanmasÄ±) sonrasÄ± bellek gÃ¼ncellenmiyor.  \n",
    "   - â€œArtÄ±k boÅŸâ€ olan hÃ¼crelerin hafÄ±zadan silinmesi veya gÃ¼ncellenmesi daha verimli olur.\n",
    "\n",
    "3. **Risk DeÄŸerlendirme (Utility-Based Decision)**  \n",
    "   - Karar mekanizmasÄ±nda sadece kaynak odaklÄ± hareket var.  \n",
    "   - Bunun yerine **beklenen kazanÃ§ (expected utility)** hesaplayan bir sistem eklenebilir:  \n",
    "     Ã¶rn. `utility = reward - risk`.\n",
    "\n",
    "4. **Enerji veya Zaman SÄ±nÄ±rlamasÄ±**  \n",
    "   - Åu an ajan sÄ±nÄ±rsÄ±z hareket ediyor.  \n",
    "   - Bir â€œenerjiâ€ veya â€œadÄ±m limitiâ€ eklenerek stratejik optimizasyon teÅŸvik edilebilir.\n",
    "\n",
    "5. **Ã–ÄŸrenme YeteneÄŸi (Learning Agent)**  \n",
    "   - Performans geÃ§miÅŸine gÃ¶re aksiyonlarÄ±nÄ± optimize eden bir Ã¶ÄŸrenme mekanizmasÄ± (Ã¶rneÄŸin Q-Learning veya SARSA) eklenebilir.  \n",
    "   - Bu sayede ajan zamanla daha verimli yollar keÅŸfedebilir.\n",
    "\n",
    "6. **Ã‡oklu Hedef Ã–nceliklendirme**  \n",
    "   - Åu anda sÃ¼t ve yumurta aynÄ± Ã¶neme sahip.  \n",
    "   - FarklÄ± kaynak tÃ¼rlerine aÄŸÄ±rlÄ±k atayan bir **Ã¶ncelik sistemi** geliÅŸtirilebilir.\n",
    "\n",
    "7. **BataklÄ±k ve AyÄ±dan KaÃ§Ä±nma**  \n",
    "   - Ajan, tehlikeli hÃ¼creleri bilse de bunlardan kaÃ§Ä±nma stratejisine sahip deÄŸil.  \n",
    "   - Bellekte â€œtehlikeli bÃ¶lgelerâ€ olarak iÅŸaretleme ve rotadan Ã§Ä±karma (avoidance logic) eklenebilir.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0884653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "HARITA (10x10)\n",
      "==================================================\n",
      "SatÄ±r 0: EEEEEEEEEE\n",
      "SatÄ±r 1: E Y   S  E\n",
      "SatÄ±r 2: E YY  S  E\n",
      "SatÄ±r 3: EC       E\n",
      "SatÄ±r 4: E     YY E\n",
      "SatÄ±r 5: E S      E\n",
      "SatÄ±r 6: E    B   E\n",
      "SatÄ±r 7: E    B   E\n",
      "SatÄ±r 8: E A      E\n",
      "SatÄ±r 9: EEEEEEEEEE\n",
      "\n",
      "Legend: E=Engel, Y=Yumurta(+2), S=SÃ¼t(+5), B=BataklÄ±k(-10), A=AyÄ±(-100), C=BaÅŸlangÄ±Ã§\n",
      "==================================================\n",
      "\n",
      "ğŸšœ SimÃ¼lasyon baÅŸlatÄ±lÄ±yor (5 x 50 adÄ±m)...\n",
      "\n",
      "ğŸ“Š DETAYLI DENEME #1:\n",
      "--------------------------------------------------\n",
      "BaÅŸlangÄ±Ã§: (3, 1), YÃ¶n: EAST\n",
      "AdÄ±m 1: FORWARD -> Pozisyon: (3, 2)\n",
      "AdÄ±m 2: LEFT -> Pozisyon: (3, 2), YÃ¶n: NORTH, Puan: +0 (Toplam: 0)\n",
      "AdÄ±m 3: FORWARD -> Pozisyon: (2, 2)\n",
      "AdÄ±m 4: LOAD -> Pozisyon: (2, 2), YÃ¶n: NORTH, Puan: +2 (Toplam: 2)\n",
      "AdÄ±m 5: FORWARD -> Pozisyon: (1, 2)\n",
      "AdÄ±m 6: LEFT -> Pozisyon: (1, 2), YÃ¶n: WEST, Puan: +0 (Toplam: 2)\n",
      "AdÄ±m 7: FORWARD -> Pozisyon: (1, 1)\n",
      "AdÄ±m 8: LEFT -> Pozisyon: (1, 1), YÃ¶n: SOUTH, Puan: +0 (Toplam: 2)\n",
      "AdÄ±m 9: FORWARD -> Pozisyon: (2, 1)\n",
      "AdÄ±m 10: FORWARD -> Pozisyon: (3, 1)\n",
      "AdÄ±m 11: FORWARD -> Pozisyon: (4, 1)\n",
      "AdÄ±m 12: FORWARD -> Pozisyon: (5, 1)\n",
      "AdÄ±m 13: LEFT -> Pozisyon: (5, 1), YÃ¶n: EAST, Puan: +0 (Toplam: 2)\n",
      "AdÄ±m 14: FORWARD -> Pozisyon: (5, 2)\n",
      "AdÄ±m 15: LOAD -> Pozisyon: (5, 2), YÃ¶n: EAST, Puan: +5 (Toplam: 7)\n",
      "AdÄ±m 16: FORWARD -> Pozisyon: (5, 3)\n",
      "AdÄ±m 17: FORWARD -> Pozisyon: (5, 4)\n",
      "AdÄ±m 18: FORWARD -> Pozisyon: (5, 5)\n",
      "AdÄ±m 19: FORWARD -> Pozisyon: (5, 6)\n",
      "AdÄ±m 20: FORWARD -> Pozisyon: (5, 7)\n",
      "AdÄ±m 21: FORWARD -> Pozisyon: (5, 8)\n",
      "AdÄ±m 22: RIGHT -> Pozisyon: (5, 8), YÃ¶n: SOUTH, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 23: FORWARD -> Pozisyon: (6, 8)\n",
      "AdÄ±m 24: FORWARD -> Pozisyon: (7, 8)\n",
      "AdÄ±m 25: FORWARD -> Pozisyon: (8, 8)\n",
      "AdÄ±m 26: LEFT -> Pozisyon: (8, 8), YÃ¶n: EAST, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 27: LEFT -> Pozisyon: (8, 8), YÃ¶n: NORTH, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 28: FORWARD -> Pozisyon: (7, 8)\n",
      "AdÄ±m 29: LEFT -> Pozisyon: (7, 8), YÃ¶n: WEST, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 30: FORWARD -> Pozisyon: (7, 7)\n",
      "AdÄ±m 31: FORWARD -> Pozisyon: (7, 6)\n",
      "AdÄ±m 32: LEFT -> Pozisyon: (7, 6), YÃ¶n: SOUTH, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 33: FORWARD -> Pozisyon: (8, 6)\n",
      "AdÄ±m 34: LEFT -> Pozisyon: (8, 6), YÃ¶n: EAST, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 35: FORWARD -> Pozisyon: (8, 7)\n",
      "AdÄ±m 36: RIGHT -> Pozisyon: (8, 7), YÃ¶n: SOUTH, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 37: LEFT -> Pozisyon: (8, 7), YÃ¶n: EAST, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 38: FORWARD -> Pozisyon: (8, 8)\n",
      "AdÄ±m 39: LEFT -> Pozisyon: (8, 8), YÃ¶n: NORTH, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 40: FORWARD -> Pozisyon: (7, 8)\n",
      "AdÄ±m 41: LEFT -> Pozisyon: (7, 8), YÃ¶n: WEST, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 42: FORWARD -> Pozisyon: (7, 7)\n",
      "AdÄ±m 43: FORWARD -> Pozisyon: (7, 6)\n",
      "AdÄ±m 44: RIGHT -> Pozisyon: (7, 6), YÃ¶n: NORTH, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 45: FORWARD -> Pozisyon: (6, 6)\n",
      "AdÄ±m 46: RIGHT -> Pozisyon: (6, 6), YÃ¶n: EAST, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 47: FORWARD -> Pozisyon: (6, 7)\n",
      "AdÄ±m 48: FORWARD -> Pozisyon: (6, 8)\n",
      "AdÄ±m 49: RIGHT -> Pozisyon: (6, 8), YÃ¶n: SOUTH, Puan: +0 (Toplam: 7)\n",
      "AdÄ±m 50: LEFT -> Pozisyon: (6, 8), YÃ¶n: EAST, Puan: +0 (Toplam: 7)\n",
      "\n",
      "Son Pozisyon: (6, 8), Son YÃ¶n: EAST\n",
      "Yumurta taÅŸÄ±yor: True, SÃ¼t taÅŸÄ±yor: True\n",
      "HafÄ±zadaki tehlike sayÄ±sÄ±: 2\n",
      "\n",
      "âœ… Deneme 1 Sonucu: 7 puan\n",
      "\n",
      "Deneme 2: 7 puan\n",
      "Deneme 3: 7 puan\n",
      "Deneme 4: 7 puan\n",
      "Deneme 5: 7 puan\n",
      "\n",
      "==================================================\n",
      "ğŸ“ˆ PERFORMANS Ã–ZETÄ°\n",
      "==================================================\n",
      "Performans DeÄŸerleri (Liste): [7, 7, 7, 7, 7]\n",
      "Ortalama Performans (5 Deneme): 7.00 puan\n",
      "En Ä°yi Performans: 7 puan\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#AGENT2 - Risk OdaklÄ± Model-TabanlÄ± Refleks Ajan\n",
    "\n",
    "\n",
    "import random\n",
    "import statistics\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Direction(Enum):\n",
    "    \"\"\"YÃ¶n tanÄ±mlarÄ±: Kuzey=0, DoÄŸu=1, GÃ¼ney=2, BatÄ±=3\"\"\"\n",
    "    NORTH = 0\n",
    "    EAST = 1\n",
    "    SOUTH = 2\n",
    "    WEST = 3\n",
    "\n",
    "# Her yÃ¶n iÃ§in hareket deltalarÄ± (row, col)\n",
    "move_delta = {\n",
    "    Direction.NORTH: (-1, 0),\n",
    "    Direction.SOUTH: (1, 0),\n",
    "    Direction.EAST: (0, 1),\n",
    "    Direction.WEST: (0, -1)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class XYEnvironment:\n",
    "    \"\"\"\n",
    "    10x10 grid ortamÄ±.\n",
    "    - 'E': Engel (geÃ§ilemez)\n",
    "    - 'Y': Yumurta (+2 puan)\n",
    "    - 'S': SÃ¼t (+5 puan)\n",
    "    - 'B': BataklÄ±k (-10 puan) -> Performans kaybÄ±\n",
    "    - 'A': AyÄ± (-100 puan) -> Performans kaybÄ±\n",
    "    - 'C': BaÅŸlangÄ±Ã§ pozisyonu\n",
    "    - ' ': BoÅŸ alan\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid, start):\n",
    "        # OrtamÄ±n baÅŸlangÄ±Ã§ durumunu sakla (simÃ¼lasyon tekrarÄ± iÃ§in)\n",
    "        self._initial_grid = [row[:] for row in grid]\n",
    "        self.start = start\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"OrtamÄ± baÅŸlangÄ±Ã§ durumuna dÃ¶ndÃ¼rÃ¼r.\"\"\"\n",
    "        self.grid = [row[:] for row in self._initial_grid]\n",
    "    \n",
    "    def in_bounds(self, pos):\n",
    "        \"\"\"Pozisyon sÄ±nÄ±rlar iÃ§inde mi?\"\"\"\n",
    "        r, c = pos\n",
    "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
    "    \n",
    "    def is_obstacle(self, pos):\n",
    "        \"\"\"Pozisyon engel mi?\"\"\"\n",
    "        r, c = pos\n",
    "        return self.grid[r][c] == \"E\"\n",
    "    \n",
    "    def percept(self, pos):\n",
    "        \"\"\"\n",
    "        KÄ±smi gÃ¶zlem: Bulunulan hÃ¼cre + 4 komÅŸu hÃ¼crenin iÃ§eriÄŸi\n",
    "        \"\"\"\n",
    "        r, c = pos\n",
    "        obs = {}\n",
    "        # Bulunulan hÃ¼crenin iÃ§eriÄŸi\n",
    "        obs['current'] = self.grid[r][c]\n",
    "        \n",
    "        # KomÅŸu hÃ¼crelerin iÃ§eriÄŸi\n",
    "        neighbours = {}\n",
    "        for d, delta in move_delta.items():\n",
    "            nr, nc = r + delta[0], c + delta[1]\n",
    "            if self.in_bounds((nr, nc)):\n",
    "                neighbours[d.name] = self.grid[nr][nc]\n",
    "            else:\n",
    "                neighbours[d.name] = \"E\" # SÄ±nÄ±r dÄ±ÅŸÄ±nÄ± engel olarak algÄ±la\n",
    "        obs['neighbours'] = neighbours\n",
    "        return obs\n",
    "    \n",
    "    def execute(self, agent, action):\n",
    "        \"\"\"\n",
    "        Aksiyonu uygula ve performans deÄŸiÅŸimini dÃ¶ndÃ¼r.\n",
    "    \n",
    "        \"\"\"\n",
    "        perf_delta = 0\n",
    "        \n",
    "        # 1. YÃ¶n DeÄŸiÅŸtirme (LEFT/RIGHT)\n",
    "        if action == \"LEFT\":\n",
    "            agent.orientation = Direction((agent.orientation.value - 1) % 4)\n",
    "            \n",
    "        elif action == \"RIGHT\":\n",
    "            agent.orientation = Direction((agent.orientation.value + 1) % 4)\n",
    "            \n",
    "        # 2. Ä°lerleme (FORWARD)\n",
    "        elif action == \"FORWARD\":\n",
    "            dr, dc = move_delta[agent.orientation]\n",
    "            newpos = (agent.pos[0] + dr, agent.pos[1] + dc)\n",
    "            \n",
    "            if not self.in_bounds(newpos) or self.is_obstacle(newpos):\n",
    "                # Engele veya sÄ±nÄ±r dÄ±ÅŸÄ±na gidilemez, yerinde kal\n",
    "                return perf_delta\n",
    "            \n",
    "            agent.pos = newpos\n",
    "            \n",
    "            # Yeni hÃ¼credeki tehlikeleri kontrol et (Ek hareket yapmadan performans kaybÄ±)\n",
    "            r, c = agent.pos\n",
    "            cell_content = self.grid[r][c]\n",
    "            if cell_content == \"B\":\n",
    "                perf_delta -= 10  # BataklÄ±k\n",
    "            elif cell_content == \"A\":\n",
    "                perf_delta -= 100  # AyÄ±\n",
    "                \n",
    "        # 3. YÃ¼kleme (LOAD)\n",
    "        elif action == \"LOAD\":\n",
    "            # Bulunulan hÃ¼credeki kaynaÄŸÄ± yÃ¼kle\n",
    "            r, c = agent.pos\n",
    "            content = self.grid[r][c]\n",
    "            \n",
    "            if content == \"Y\" and not agent.carry_egg:\n",
    "                agent.carry_egg = True\n",
    "                self.grid[r][c] = \" \"  # YumurtayÄ± haritadan kaldÄ±r\n",
    "                perf_delta += 2\n",
    "            elif content == \"S\" and not agent.carry_milk:\n",
    "                agent.carry_milk = True\n",
    "                self.grid[r][c] = \" \"  # SÃ¼tÃ¼ haritadan kaldÄ±r\n",
    "                perf_delta += 5\n",
    "        \n",
    "        return perf_delta\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Model-based reflex agent: HafÄ±za ve tehlike bilgisi kullanarak karar verir.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, start, orientation=Direction.EAST):\n",
    "        self.start = start\n",
    "        self.initial_orientation = orientation\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"AjanÄ± baÅŸlangÄ±Ã§ durumuna dÃ¶ndÃ¼rÃ¼r.\"\"\"\n",
    "        self.pos = self.start\n",
    "        self.orientation = self.initial_orientation\n",
    "        self.carry_egg = False\n",
    "        self.carry_milk = False\n",
    "        self.memory = {}  # GÃ¶zlemlenen hÃ¼crelerin iÃ§eriÄŸi\n",
    "        self.dangers = set()  # Tehlikeli hÃ¼creler (ayÄ± ve bataklÄ±k)\n",
    "        self.visited = set()  # Ziyaret edilen hÃ¼creler\n",
    "        self.visited.add(self.start)\n",
    "    \n",
    "    def perceive(self, env):\n",
    "        \"\"\"Ortamdan algÄ±lama yap ve hafÄ±zaya kaydet\"\"\"\n",
    "        p = env.percept(self.pos)\n",
    "        \n",
    "        # Bulunulan hÃ¼creyi hafÄ±zaya kaydet/gÃ¼ncelle\n",
    "        r, c = self.pos\n",
    "        self.memory[(r, c)] = p['current']\n",
    "        \n",
    "        # KomÅŸularÄ± hafÄ±zaya kaydet/gÃ¼ncelle\n",
    "        for dname, content in p['neighbours'].items():\n",
    "            if content is not None:\n",
    "                dir_enum = Direction[dname]\n",
    "                dr, dc = move_delta[dir_enum]\n",
    "                nr, nc = r + dr, c + dc\n",
    "                \n",
    "                # SÄ±nÄ±r dÄ±ÅŸÄ±/Engel deÄŸilse (None veya \"E\" deÄŸilse)\n",
    "                if env.in_bounds((nr, nc)) and not env.is_obstacle((nr, nc)):\n",
    "                    self.memory[(nr, nc)] = content\n",
    "                \n",
    "                # Tehlikeleri iÅŸaretle\n",
    "                if content in ('A', 'B'):\n",
    "                    self.dangers.add((nr, nc))\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def is_safe(self, pos):\n",
    "        \n",
    "        return pos not in self.dangers\n",
    "    \n",
    "    def program(self, percept, env):\n",
    "       \n",
    "        cur = percept['current']\n",
    "        \n",
    "        # 1. AmaÃ§: BulunduÄŸu yerde kaynak varsa yÃ¼kle\n",
    "        if (cur == \"S\" and not self.carry_milk) or \\\n",
    "           (cur == \"Y\" and not self.carry_egg):\n",
    "            return \"LOAD\"\n",
    "        \n",
    "        # Mevcut yÃ¶n:\n",
    "        dr, dc = move_delta[self.orientation]\n",
    "        forward_pos = (self.pos[0] + dr, self.pos[1] + dc)\n",
    "        \n",
    "        # 2. Tehlike KontrolÃ¼: Ä°lerideki hÃ¼cre tehlikeli, engel veya sÄ±nÄ±r dÄ±ÅŸÄ± mÄ±?\n",
    "        forward_safe = env.in_bounds(forward_pos) and \\\n",
    "                       not env.is_obstacle(forward_pos) and \\\n",
    "                       self.is_safe(forward_pos)\n",
    "        \n",
    "        if not forward_safe:\n",
    "            # Ä°leride engel, sÄ±nÄ±r veya tehlike var, dÃ¶n\n",
    "            return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "            \n",
    "        # 3. AmaÃ§: GÃ¼venli kaynaklara yÃ¶nel\n",
    "        targets = []\n",
    "        for pos, content in self.memory.items():\n",
    "            if pos in self.dangers: continue # GÃ¼venli olmayan hedefleri atla\n",
    "                \n",
    "            if content == \"S\" and not self.carry_milk:\n",
    "                targets.append((pos, 5)) # SÃ¼t Ã¶ncelik 5\n",
    "            elif content == \"Y\" and not self.carry_egg:\n",
    "                targets.append((pos, 2)) # Yumurta Ã¶ncelik 2\n",
    "                \n",
    "        if targets:\n",
    "          \n",
    "            target_pos = max(targets, key=lambda t: (\n",
    "                t[1], # Ã–nce deÄŸer\n",
    "                - (abs(t[0][0] - self.pos[0]) + abs(t[0][1] - self.pos[1])) # Sonra yakÄ±nlÄ±k\n",
    "            ))[0]\n",
    "            \n",
    "            # Hedefe doÄŸru yÃ¶nelim belirle\n",
    "            action = self.get_action_to_target(target_pos)\n",
    "            \n",
    "            # EÄŸer aksiyon ileri gitmekse ve gÃ¼venliyse uygula\n",
    "            if action == \"FORWARD\":\n",
    "                return \"FORWARD\"\n",
    "            # DÃ¶nme eylemi ise, dÃ¶n ve yeni ileri pozisyonu kontrol et\n",
    "            elif action in (\"LEFT\", \"RIGHT\"):\n",
    "                return action\n",
    "            # HiÃ§bir ÅŸey yapÄ±lamÄ±yorsa (hedef yanÄ±mÄ±zda ama ileri gidemiyoruz)\n",
    "            else:\n",
    "                return random.choice([\"LEFT\", \"RIGHT\"]) # Rastgele dÃ¶n\n",
    "        \n",
    "        # 4. KeÅŸif: GÃ¼venli ve ziyaret edilmemiÅŸ hÃ¼crelere doÄŸru ilerle\n",
    "        if forward_safe:\n",
    "            if forward_pos not in self.visited:\n",
    "                self.visited.add(forward_pos)\n",
    "                return \"FORWARD\"\n",
    "            # Ziyaret edilmiÅŸ gÃ¼venli bir yolda %50 ÅŸansla ilerle (dÃ¶ngÃ¼den kaÃ§Ä±nma)\n",
    "            elif random.random() < 0.5:\n",
    "                return \"FORWARD\"\n",
    "                \n",
    "        # 5. Son Ã§are: Rastgele gÃ¼venli bir yÃ¶ne dÃ¶n\n",
    "        return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "    \n",
    "    def get_action_to_target(self, target_pos):\n",
    "        \"\"\"Hedefe doÄŸru dÃ¶nmek iÃ§in gerekli eylemi bulur.\"\"\"\n",
    "        dr = target_pos[0] - self.pos[0]\n",
    "        dc = target_pos[1] - self.pos[1]\n",
    "        \n",
    "        # Ã–ncelikli hareket yÃ¶nÃ¼nÃ¼ belirle\n",
    "        desired_dir = None\n",
    "        if abs(dr) > abs(dc): # Dikey hareket Ã¶ncelikli\n",
    "            desired_dir = Direction.NORTH if dr < 0 else Direction.SOUTH\n",
    "        elif abs(dc) > 0: # Yatay hareket Ã¶ncelikli (dr=0 olabilir)\n",
    "            desired_dir = Direction.WEST if dc < 0 else Direction.EAST\n",
    "        else: # Zaten hedefteyiz (LOAD'dan sonra buraya dÃ¼ÅŸebiliriz)\n",
    "            return \"LOAD\" \n",
    "            \n",
    "        # Mevcut yÃ¶nden hedef yÃ¶ne dÃ¶nme eylemini hesapla\n",
    "        if desired_dir is not None:\n",
    "            diff = (desired_dir.value - self.orientation.value) % 4\n",
    "            \n",
    "            if diff == 0:\n",
    "                return \"FORWARD\"\n",
    "            elif diff == 1:\n",
    "                return \"RIGHT\"\n",
    "            elif diff == 3:\n",
    "                return \"LEFT\"\n",
    "            else: # diff == 2 (Ters yÃ¶n)\n",
    "                return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "        \n",
    "        return random.choice([\"LEFT\", \"RIGHT\"]) # GÃ¼venlik\n",
    "\n",
    "\n",
    "\n",
    "def create_grid():\n",
    "    \"\"\"10x10 grid haritasÄ± oluÅŸtur\"\"\"\n",
    "    # ... (AynÄ± harita tanÄ±mÄ±) ...\n",
    "    grid = [[\"E\"] * 10 for _ in range(10)]\n",
    "    \n",
    "    for i in range(1, 9):\n",
    "        for j in range(1, 9):\n",
    "            grid[i][j] = \" \"\n",
    "    \n",
    "    placements = {\n",
    "        (1, 2): \"Y\", (2, 2): \"Y\", (2, 3): \"Y\", \n",
    "        (1, 6): \"S\", (2, 6): \"S\", \n",
    "        (5, 2): \"S\", \n",
    "        (4, 6): \"Y\", (4, 7): \"Y\", \n",
    "        (7, 5): \"B\", (6, 5): \"B\", \n",
    "        (8, 2): \"A\" \n",
    "    }\n",
    "    \n",
    "    for (r, c), value in placements.items():\n",
    "        grid[r][c] = value\n",
    "    \n",
    "    start = (3, 1)\n",
    "    grid[start[0]][start[1]] = \"C\"\n",
    "    \n",
    "    return grid, start\n",
    "\n",
    "def run_episode(grid, start, max_steps=50, verbose=False):\n",
    "    \"\"\"Bir epizodu Ã§alÄ±ÅŸtÄ±r ve toplam performansÄ± dÃ¶ndÃ¼r\"\"\"\n",
    "    env = XYEnvironment(grid, start)\n",
    "    agent = Agent(start, orientation=Direction.EAST)\n",
    "    total_perf = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"BaÅŸlangÄ±Ã§: {start}, YÃ¶n: {agent.orientation.name}\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        percept = agent.perceive(env)\n",
    "        action = agent.program(percept, env)\n",
    "        perf_delta = env.execute(agent, action)\n",
    "        total_perf += perf_delta\n",
    "        \n",
    "        if verbose:\n",
    "            # Sadece LOAD, FORWARD (Bat/AyÄ±) veya yÃ¶n deÄŸiÅŸtirmeler iÃ§in Ã§Ä±ktÄ± ver\n",
    "            if action == \"LOAD\" or (action == \"FORWARD\" and perf_delta != 0) or action in (\"LEFT\", \"RIGHT\"):\n",
    "                print(f\"AdÄ±m {step+1}: {action} -> Pozisyon: {agent.pos}, YÃ¶n: {agent.orientation.name}, Puan: {perf_delta:+d} (Toplam: {total_perf})\")\n",
    "            elif action == \"FORWARD\":\n",
    "                 print(f\"AdÄ±m {step+1}: {action} -> Pozisyon: {agent.pos}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nSon Pozisyon: {agent.pos}, Son YÃ¶n: {agent.orientation.name}\")\n",
    "        print(f\"Yumurta taÅŸÄ±yor: {agent.carry_egg}, SÃ¼t taÅŸÄ±yor: {agent.carry_milk}\")\n",
    "        print(f\"HafÄ±zadaki tehlike sayÄ±sÄ±: {len(agent.dangers)}\")\n",
    "    \n",
    "    return total_perf\n",
    "\n",
    "\n",
    "\n",
    "GRID, START = create_grid()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"HARITA (10x10)\")\n",
    "print(\"=\" * 50)\n",
    "for i, row in enumerate(GRID):\n",
    "    print(f\"SatÄ±r {i}: {''.join(row)}\")\n",
    "print(\"\\nLegend: E=Engel, Y=Yumurta(+2), S=SÃ¼t(+5), B=BataklÄ±k(-10), A=AyÄ±(-100), C=BaÅŸlangÄ±Ã§\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "performances = []\n",
    "NUM_EPISODES = 5\n",
    "MAX_STEPS = 50\n",
    "\n",
    "print(f\"\\nğŸšœ SimÃ¼lasyon baÅŸlatÄ±lÄ±yor ({NUM_EPISODES} x {MAX_STEPS} adÄ±m)...\")\n",
    "\n",
    "# Her denemede ortamÄ± ve ajanÄ± sÄ±fÄ±rla\n",
    "for i in range(NUM_EPISODES):\n",
    "    # Ä°lk denemeyi detaylÄ± gÃ¶ster\n",
    "    verbose = (i == 0)\n",
    "    if verbose:\n",
    "        print(f\"\\n DETAYLI DENEME #{i+1}:\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "   \n",
    "    perf = run_episode(GRID, START, max_steps=MAX_STEPS, verbose=verbose)\n",
    "    performances.append(perf)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n Deneme {i+1} Sonucu: {perf} puan\\n\")\n",
    "    else:\n",
    "        print(f\"Deneme {i+1}: {perf} puan\")\n",
    "\n",
    "mean_perf = statistics.mean(performances)\n",
    "max_perf = max(performances)\n",
    "min_perf = min(performances)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ğŸ“ˆ PERFORMANS Ã–ZETÄ°\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Performans DeÄŸerleri (Liste): {performances}\")\n",
    "print(f\"Ortalama Performans ({NUM_EPISODES} Deneme): {mean_perf:.2f} puan\")\n",
    "print(f\"En Ä°yi Performans: {max_perf} puan\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d33bd",
   "metadata": {},
   "source": [
    "#  Agent 1 vs Agent 2 \n",
    "\n",
    "##  Genel KarÅŸÄ±laÅŸtÄ±rma\n",
    "\n",
    "| Ã–zellik | **Agent 1** | **Agent 2** | **Fark / Ä°yileÅŸme NoktasÄ±** |\n",
    "|----------|--------------|--------------|------------------------------|\n",
    "| **Ajan Tipi** | Model-Based Reflex Agent | Model-Based Reflex Agent | AynÄ± temel yapÄ± korunmuÅŸ. |\n",
    "| **Bellek KullanÄ±mÄ±** | Sadece gÃ¶zlemlenen hÃ¼creleri kaydeder. | HÃ¼crelerin durumuna gÃ¶re ayrÄ±ca â€œtehlike (A, B)â€ etiketleri ekler. | Agent 2, **tehlike bilinci** eklemiÅŸtir. |\n",
    "| **AlgÄ± YeteneÄŸi** | BulunduÄŸu hÃ¼cre + 4 komÅŸu | BulunduÄŸu hÃ¼cre + 4 komÅŸu | AynÄ±. |\n",
    "| **Karar MekanizmasÄ±** | Sadece kaynaklara yÃ¶nelir veya rastgele keÅŸif yapar. | Ã–ncelik sÄ±rasÄ± belirlenmiÅŸtir (LOAD > KaÃ§Ä±nma > Hedefe ilerleme > KeÅŸif). | Agent 2â€™de **Ã¶ncelik tabanlÄ± karar mantÄ±ÄŸÄ±** eklenmiÅŸ. |\n",
    "| **Hedef SeÃ§imi** | Manhattan mesafesine gÃ¶re en yakÄ±n hedef | Ã–dÃ¼l (puan) + mesafe birlikte deÄŸerlendirilir | Agent 2, **Ã¶dÃ¼l-aÄŸÄ±rlÄ±klÄ± hedef seÃ§imi** yapÄ±yor. |\n",
    "| **Tehlike YÃ¶netimi** | Tehlikelerden kaÃ§Ä±nma stratejisi yok. | Tehlikeli hÃ¼creleri hafÄ±zada iÅŸaretler ve tekrar girmekten kaÃ§Ä±nÄ±r. | Agent 2, **aktif risk takibi** iÃ§eriyor. |\n",
    "| **Performans Ã–lÃ§Ã¼tÃ¼** | Kaynak ve tehlike puanlarÄ± tanÄ±mlÄ± | AynÄ± puan sistemi + istatistiksel analiz | Agent 2â€™de **epizodik test ve performans analizi** var. |\n",
    "| **Planlama / Yol Bulma** | HenÃ¼z yok (sadece Manhattan mesafesi) | HenÃ¼z yok ama geliÅŸtirme Ã¶nerilerinde daha net tanÄ±mlanmÄ±ÅŸ | Her iki ajan da henÃ¼z planlama yapmÄ±yor, ancak Agent 2 bu yÃ¶nde daha sistematik. |\n",
    "| **Ã–ÄŸrenme YeteneÄŸi** | HenÃ¼z yok (Ã¶neri olarak belirtilmiÅŸ) | HenÃ¼z yok (Ã¶neri olarak belirtilmiÅŸ) | Benzer. |\n",
    "| **GÃ¶rselleÅŸtirme / Analiz** | Yok | Ã–neri olarak var | Agent 2, **analiz & gÃ¶rselleÅŸtirme odaklÄ±** dÃ¼ÅŸÃ¼nÃ¼lmÃ¼ÅŸ. |\n",
    "\n",
    "---\n",
    "\n",
    "##  Temel FarklarÄ±n Ã–zeti\n",
    "\n",
    "1. **Agent 2, Agent 1â€™in geliÅŸtirilmiÅŸ bir sÃ¼rÃ¼mÃ¼dÃ¼r.**  \n",
    "   Agent 1 yalnÄ±zca Ã§evreyi gÃ¶zlemleyip hedefe yÃ¶nelirken, Agent 2 bu bilgiyi **tehlike bilinci** ve **Ã¶ncelik mantÄ±ÄŸÄ±**yla birleÅŸtirir.\n",
    "\n",
    "2. **Karar verme sÃ¼reci Agent 2â€™de daha katmanlÄ±dÄ±r.**  \n",
    "   Ajan, gÃ¼venlik â†’ hedef â†’ keÅŸif sÄ±ralamasÄ±nda karar alÄ±r.\n",
    "\n",
    "3. **Hedef seÃ§imi artÄ±k daha rasyonel.**  \n",
    "   Agent 2, sadece yakÄ±nlÄ±k deÄŸil, **Ã¶dÃ¼l deÄŸerini** de dikkate alÄ±r.\n",
    "\n",
    "4. **Risk yÃ¶netimi eklendi.**  \n",
    "   Tehlikeli hÃ¼creler (AyÄ±, BataklÄ±k) hafÄ±zada tutulur ve kaÃ§Ä±nÄ±lÄ±r â€” Agent 1 bunu yapmÄ±yordu.\n",
    "\n",
    "5. **Deneysel deÄŸerlendirme yapÄ±sÄ± eklenmiÅŸ.**  \n",
    "   Agent 2, epizot bazlÄ± testlerle performansÄ±nÄ± Ã¶lÃ§er ve kÄ±yaslar.\n",
    "\n",
    "---\n",
    "\n",
    "##  SonuÃ§\n",
    "\n",
    "> **Agent 2**, Agent 1â€™in temel prensiplerini koruyarak Ã¼zerine ÅŸu Ã¼Ã§ Ã¶nemli katmanÄ± eklemiÅŸtir:  \n",
    "> - **Risk bilinci (tehlike takibi)**  \n",
    "> - **Ã–ncelik tabanlÄ± karar mantÄ±ÄŸÄ±**  \n",
    "> - **Ã–dÃ¼l + mesafe odaklÄ± hedef seÃ§imi**  \n",
    "\n",
    "Bu da Agent 2â€™yi daha **akÄ±llÄ±**, **uyanÄ±k** ve **stratejik** bir ajan haline getirmiÅŸtir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f6211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deneme 1: 5 puan\n",
      "Deneme 2: 10 puan\n",
      "Deneme 3: -70 puan\n",
      "Deneme 4: -1100 puan\n",
      "Deneme 5: 0 puan\n",
      "\n",
      "Ortalama Performans: -231.00\n"
     ]
    }
   ],
   "source": [
    "#AGENT3-  Rastgele Hareketli Ajan\n",
    "\n",
    "import random\n",
    "\n",
    "class XYEnvironment:\n",
    "    def __init__(self, width=10, height=10):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grid = [[' ' for _ in range(width)] for _ in range(height)]\n",
    "        self.populate_environment()\n",
    "\n",
    "    def populate_environment(self):\n",
    "        # Rastgele konumlara yumurta, sÃ¼t, bataklÄ±k ve ayÄ± yerleÅŸtir ! \n",
    "        items = ['Y', 'S', 'B', 'A']\n",
    "        for item in items:\n",
    "            for _ in range(random.randint(3, 5)):\n",
    "                x, y = random.randint(0, self.width - 1), random.randint(0, self.height - 1)\n",
    "                self.grid[y][x] = item\n",
    "\n",
    "        # BaÅŸlangÄ±Ã§ pozisyonu\n",
    "        self.start = (random.randint(0, self.width - 1), random.randint(0, self.height - 1))\n",
    "        self.grid[self.start[1]][self.start[0]] = 'C'\n",
    "\n",
    "    def get_cell(self, x, y):\n",
    "        if 0 <= x < self.width and 0 <= y < self.height:\n",
    "            return self.grid[y][x]\n",
    "        else:\n",
    "            return 'E'  # Engel sÄ±nÄ±r dÄ±ÅŸÄ±nda\n",
    "\n",
    "    def set_cell(self, x, y, value):\n",
    "        if 0 <= x < self.width and 0 <= y < self.height:\n",
    "            self.grid[y][x] = value\n",
    "\n",
    "\n",
    "\n",
    "class Direction:\n",
    "    NORTH, EAST, SOUTH, WEST = 0, 1, 2, 3\n",
    "\n",
    "    @staticmethod\n",
    "    def turn_left(direction):\n",
    "        return (direction - 1) % 4\n",
    "\n",
    "    @staticmethod\n",
    "    def turn_right(direction):\n",
    "        return (direction + 1) % 4\n",
    "\n",
    "class FarmerAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.x, self.y = env.start\n",
    "        self.direction = random.choice([0, 1, 2, 3])\n",
    "        self.performance = 0\n",
    "        self.loaded = False\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.env.get_cell(self.x, self.y)\n",
    "\n",
    "    def move_forward(self):\n",
    "        if self.direction == Direction.NORTH:\n",
    "            new_y = self.y - 1\n",
    "            new_x = self.x\n",
    "        elif self.direction == Direction.SOUTH:\n",
    "            new_y = self.y + 1\n",
    "            new_x = self.x\n",
    "        elif self.direction == Direction.EAST:\n",
    "            new_x = self.x + 1\n",
    "            new_y = self.y\n",
    "        else:\n",
    "            new_x = self.x - 1\n",
    "            new_y = self.y\n",
    "\n",
    "        if self.env.get_cell(new_x, new_y) != 'E':\n",
    "            self.x, self.y = new_x, new_y\n",
    "\n",
    "    def turn_left(self):\n",
    "        self.direction = Direction.turn_left(self.direction)\n",
    "\n",
    "    def turn_right(self):\n",
    "        self.direction = Direction.turn_right(self.direction)\n",
    "\n",
    "    def load(self):\n",
    "        cell = self.perceive()\n",
    "        if cell == 'Y':\n",
    "            self.performance += 2\n",
    "            self.env.set_cell(self.x, self.y, ' ')\n",
    "        elif cell == 'S':\n",
    "            self.performance += 5\n",
    "            self.env.set_cell(self.x, self.y, ' ')\n",
    "        self.loaded = True\n",
    "\n",
    "    def step(self):\n",
    "        # AlgÄ±la\n",
    "        cell = self.perceive()\n",
    "\n",
    "        # Ortam etkileri\n",
    "        if cell == 'B':\n",
    "            self.performance -= 10\n",
    "        elif cell == 'A':\n",
    "            self.performance -= 100\n",
    "\n",
    "        # Karar verme (basit rastgele hareket)\n",
    "        action = random.choice(['move', 'left', 'right', 'load'])\n",
    "        if action == 'move':\n",
    "            self.move_forward()\n",
    "        elif action == 'left':\n",
    "            self.turn_left()\n",
    "        elif action == 'right':\n",
    "            self.turn_right()\n",
    "        elif action == 'load':\n",
    "            self.load()\n",
    "\n",
    "    def run(self, steps=50):\n",
    "        for _ in range(steps):\n",
    "            self.step()\n",
    "        return self.performance\n",
    "\n",
    "\n",
    "\n",
    "performances = []\n",
    "\n",
    "for i in range(5):\n",
    "    env = XYEnvironment()\n",
    "    agent = FarmerAgent(env)\n",
    "    perf = agent.run(50)\n",
    "    performances.append(perf)\n",
    "    print(f\"Deneme {i+1}: {perf} puan\")\n",
    "\n",
    "ortalama = sum(performances) / len(performances)\n",
    "print(f\"\\nOrtalama Performans: {ortalama:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9431fea",
   "metadata": {},
   "source": [
    "# Agent3 - Rastgele Hareketli Model TabanlÄ± Refleks AjanÄ±  \n",
    "### (Agent1 ve Agent2 ile KarÅŸÄ±laÅŸtÄ±rmasÄ±)\n",
    "\n",
    "##  Genel KarÅŸÄ±laÅŸtÄ±rma Tablosu\n",
    "\n",
    "| Ã–zellik | **Agent1** | **Agent2** | **Agent3** | **Fark / GeliÅŸim DÃ¼zeyi** |\n",
    "|----------|-------------|-------------|-------------|-----------------------------|\n",
    "| **Ajan Tipi** | Model-Based Reflex | Model-Based Reflex | Model-Based Reflex | Temel yapÄ± aynÄ±, ancak zekÃ¢ dÃ¼zeyi farklÄ±. |\n",
    "| **Karar MekanizmasÄ±** | Hedef odaklÄ± (S, Y hÃ¼crelerine yÃ¶nelir) | Hedef + risk odaklÄ±, Ã¶ncelikli karar mantÄ±ÄŸÄ± | **Tamamen rastgele karar verir** | Agent3â€™te bilinÃ§li karar mekanizmasÄ± yok. |\n",
    "| **Bellek KullanÄ±mÄ±** | GÃ¶zlemlenen hÃ¼creleri kaydeder | HÃ¼creleri + tehlike (A, B) bilgisiyle hafÄ±zada tutar | **Bellek yok** | Agent3, â€œmodel-basedâ€ yapÄ±da tanÄ±mlÄ± olsa da pratikte hafÄ±zasÄ±z Ã§alÄ±ÅŸÄ±r. |\n",
    "| **Hedef SeÃ§imi** | Bilinen kaynaklara yÃ¶nelir | Ã–dÃ¼l (puan) + mesafeye gÃ¶re hedef seÃ§er | **HiÃ§ hedef belirlemez** | Agent3, tamamen rastgele hareket eder. |\n",
    "| **Tehlike YÃ¶netimi** | Tehlikeleri fark eder ama kaÃ§Ä±nmaz | Tehlikeleri hafÄ±zada tutar ve kaÃ§Ä±nÄ±r | **Tehlikelerden kaÃ§Ä±nmaz** | Agent3, bataklÄ±k ve ayÄ±ya girebilir. |\n",
    "| **Performans Ã–lÃ§Ã¼tÃ¼** | Kaynak toplama ve tehlike cezasÄ± | AynÄ± Ã¶lÃ§Ã¼t + epizodik istatistik | **AynÄ± Ã¶lÃ§Ã¼t, ama rastgelelik nedeniyle tutarsÄ±z sonuÃ§lar** | Agent3 performansÄ± yÃ¼ksek varyanslÄ±dÄ±r. |\n",
    "| **YÃ¶nelim Sistemi** | `Direction` enum ile yÃ¶n deÄŸiÅŸtirir | AynÄ± | AynÄ± | Ortak sistem kullanÄ±lmÄ±ÅŸ. |\n",
    "| **Ã–ÄŸrenme YeteneÄŸi** | HenÃ¼z yok (Ã¶neri olarak belirtilmiÅŸ) | HenÃ¼z yok (Ã¶neri olarak belirtilmiÅŸ) | **Yok** | ÃœÃ§Ã¼ de Ã¶ÄŸrenmiyor, ancak Agent3 tamamen kÃ¶r. |\n",
    "| **Planlama / Yol Bulma** | HenÃ¼z yok, Ã¶neri olarak geÃ§iyor | HenÃ¼z yok, Ã¶neri olarak geÃ§iyor | **Yok** | Agent3â€™te hiÃ§bir planlama yok. |\n",
    "| **Strateji TÃ¼rÃ¼** | Hedef odaklÄ± refleks | Risk + hedef + Ã¶ncelik odaklÄ± refleks | **Rastgele hareket** | Agent3 saf â€œrandom explorationâ€ uygular. |\n",
    "\n",
    "---\n",
    "\n",
    "##  Agent3â€™Ã¼n Temel Ã–zellikleri\n",
    "\n",
    "1. **Rastgele DavranÄ±ÅŸ**  \n",
    "   Ajan, her adÄ±mda `move`, `left`, `right` veya `load` aksiyonlarÄ±ndan birini tamamen rastgele seÃ§er.  \n",
    "   Bu nedenle yÃ¶nelimi veya stratejik karar mantÄ±ÄŸÄ± yoktur.\n",
    "\n",
    "2. **Bellek veya Hedef Takibi Yok**  \n",
    "   Ã–nceki adÄ±mlarda topladÄ±ÄŸÄ± bilgi veya algÄ±larÄ±nÄ± hatÄ±rlamaz.  \n",
    "   â€œModel-Based Reflexâ€ olarak tanÄ±mlansa da fiilen **modeli yoktur.**\n",
    "\n",
    "3. **PerformansÄ±n DeÄŸiÅŸkenliÄŸi**  \n",
    "   Rastgele kararlar nedeniyle performans bÃ¼yÃ¼k oranda ÅŸansa baÄŸlÄ±dÄ±r.  \n",
    "   BazÄ± epizotlarda yÃ¼ksek puan toplayabilirken, bazÄ±larÄ±nda hÄ±zla puan kaybeder.\n",
    "\n",
    "4. **Basit Ama KarÅŸÄ±laÅŸtÄ±rma Ä°Ã§in Ã–nemli**  \n",
    "   Agent3, **taban seviye kontrol ajanÄ±** olarak kullanÄ±labilir.  \n",
    "   Agent1 ve Agent2â€™nin baÅŸarÄ±mÄ±, bu rastgele davranan ajanla karÅŸÄ±laÅŸtÄ±rÄ±larak Ã¶lÃ§Ã¼lebilir.\n",
    "\n",
    "---\n",
    "\n",
    "##  SonuÃ§\n",
    "\n",
    "> **Agent3**, karar mekanizmasÄ± veya bellek iÃ§ermeyen, tamamen **rastgele hareket eden** bir ajan modelidir.  \n",
    "> Bu yapÄ±, Agent1 ve Agent2â€™nin zekÃ¢ dÃ¼zeyini karÅŸÄ±laÅŸtÄ±rmak iÃ§in â€œkontrol grubuâ€ gÃ¶revi gÃ¶rebilir.\n",
    "\n",
    "**Ã–zetle:**\n",
    "- Agent1 â†’ Hedef odaklÄ± refleks ajan  \n",
    "- Agent2 â†’ Risk ve Ã¶dÃ¼l bilinci olan refleks ajan  \n",
    "- Agent3 â†’ Rastgele aksiyon alan basit ajan  \n",
    "\n",
    "Bu farklar, ajanÄ±n karar verme yeteneÄŸi arttÄ±kÃ§a performansÄ±n **daha tutarlÄ± ve verimli** hale geldiÄŸini gÃ¶stermektedir.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
