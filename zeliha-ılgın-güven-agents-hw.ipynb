{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8fc7b3a",
   "metadata": {},
   "source": [
    "#  Çiftçi Ajan Projesi\n",
    "\n",
    "Bu proje, **model-tabanlı refleks ajanların** kısmi gözlemlenebilir bir çiftlik ortamında nasıl farklı stratejilerle karar verdiklerini simüle eder.  \n",
    "Ajanlar, **yumurta (Y)** ve **süt (S)** toplarken **bataklık (B)** ve **ayı (A)** gibi tehlikelerden kaçınarak **maksimum performans puanı** elde etmeyi hedefler.\n",
    "\n",
    "Proje kapsamında üç farklı ajan geliştirilmiştir:\n",
    "\n",
    "| Ajan | Davranış Tipi | Kısaca |\n",
    "|------|----------------|--------|\n",
    "| **Agent1** | Temel Model-Tabanlı Refleks Ajan | Hedef odaklı, planlı hareket eder |\n",
    "| **Agent2** | Gelişmiş Model-Tabanlı Refleks Ajan | Tehlikeleri belleğinde tutar, riskten kaçınır |\n",
    "| **Agent3** | Rastgele Model-Tabanlı Refleks Ajan | Model yapısını korur fakat rastgele kararlar verir |\n",
    "\n",
    "---\n",
    "\n",
    "##  Amaç (PEAS Tanımı)\n",
    "\n",
    "| Bileşen | Açıklama |\n",
    "|----------|-----------|\n",
    "| **Performance (P)** | Yumurta toplamak **+2**, süt toplamak **+5**, bataklığa girmek **-10**, ayıya girmek **-100** puan etkiler. |\n",
    "| **Environment (E)** | 10×10 ızgara harita. Kaynaklar (S, Y) ve tehlikeler (A, B) sabit konumludur. |\n",
    "| **Actuators (A)** | `FORWARD`, `LEFT`, `RIGHT`, `LOAD` hareketleri. |\n",
    "| **Sensors (S)** | Ajan, bulunduğu hücre ve dört komşunun durumunu algılayabilir. |\n",
    "\n",
    "---\n",
    "\n",
    "##  Ortam Özellikleri\n",
    "\n",
    "| Özellik | Değer |\n",
    "|----------|--------|\n",
    "| Gözlemlenebilirlik | **Kısmi** – yalnızca mevcut konum ve komşu hücreler algılanır |\n",
    "| Deterministik | **Evet** – aynı hareket her zaman aynı sonucu üretir |\n",
    "| Dinamiklik | **Statik** – çevre zamanla değişmez |\n",
    "| Ajan Sayısı | **Tek ajan** |\n",
    "| Episodiklik | **Sekansiyel** – geçmiş kararlar sonraki adımları etkiler |\n",
    "\n",
    "---\n",
    "\n",
    "##  Ajan Türleri ve Davranış Farkları\n",
    "\n",
    "###  **Agent1 – Temel Model-Tabanlı Refleks Ajan**\n",
    "\n",
    "- Çevreyi algılar ve **gözlem geçmişini belleğinde tutar.**\n",
    "- Bilinen kaynaklara yönelir, yoksa rastgele keşif yapar.\n",
    "- Tehlike bilgisini dikkate almaz.\n",
    "\n",
    "**Karar Mekanizması:**\n",
    "1. Bulunduğu hücrede kaynak varsa → **LOAD**  \n",
    "2. Hafızasında bilinen kaynak varsa → **Yöne dön ve ilerle**  \n",
    "3. Bilgi yoksa → **Rastgele keşfet (öncelikli ileri)**  \n",
    "\n",
    "---\n",
    "\n",
    "###  **Agent2 – Risk Odaklı Model-Tabanlı Refleks Ajan**\n",
    "\n",
    "- Agent1’in tüm özelliklerine ek olarak:\n",
    "  - **Tehlikeli hücreleri (A, B)** hafızaya kaydeder.\n",
    "  - Riskli yönleri tercih etmez.\n",
    "- Daha güvenli bir rota belirler, verimlilik artar.\n",
    "\n",
    "**Ek Farklar:**\n",
    "- Tehlikeden kaçınmak için hafızadaki bilgiye dayanır.  \n",
    "- Öncelikli hedef: **Kaynak toplamak + riskten kaçınmak.**\n",
    "\n",
    "---\n",
    "\n",
    "###  **Agent3 – Rastgele Hareketli Ajan**\n",
    "\n",
    "- Model tabanlı yapıya sahip olsa da **bellek veya karar mantığı yoktur.**\n",
    "- Her adımda rastgele yön ve hareket seçer.\n",
    "- **Zekâ seviyesi düşüktür**, ancak “davranış testi” için kullanılır.\n",
    "\n",
    "**Amaç:**  \n",
    "Rastgele hareket eden bir ajanın performansını ölçerek, refleks ve bellekli ajanlarla karşılaştırmak.\n",
    "\n",
    "---\n",
    "\n",
    "##  Karşılaştırmalı Özellik Tablosu\n",
    "\n",
    "| Özellik | Agent1 | Agent2 | Agent3 |\n",
    "|----------|---------|---------|---------|\n",
    "| Model Tabanlılık | ✅ | ✅ | ✅ |\n",
    "| Bellek / Hafıza | 🟡 (Kaynak) | 🟢 (Kaynak + Tehlike) | ❌ |\n",
    "| Karar Mekanizması | Hedef odaklı | Hedef + risk analizi | Rastgele |\n",
    "| Öğrenme Yeteneği | ❌ | ❌ | ❌ |\n",
    "| Zekâ Seviyesi | Orta | Yüksek | Düşük |\n",
    "| Performans Beklentisi | Orta | Yüksek | Düşük |\n",
    "| Rastgelelik | Düşük | Düşük | Yüksek |\n",
    "\n",
    "---\n",
    "\n",
    "##  Kod Yapısı\n",
    "\n",
    "| Dosya / Sınıf | Açıklama |\n",
    "|----------------|-----------|\n",
    "| `XYEnvironment` | 10×10 ortamın haritasını, sınırları, kaynakları ve tehlikeleri yönetir. |\n",
    "| `Agent1` / `Agent2` / `Agent3` | Farklı stratejilerle karar veren ajan sınıfları. |\n",
    "| `create_grid()` | Haritayı oluşturur; kaynak ve tehlike yerleşimlerini belirler. |\n",
    "| `run_episode()` | Simülasyonu başlatır, ajan hareketlerini çalıştırır ve toplam puanı döndürür. |\n",
    "| `display_grid()` | Simülasyonun görsel çıktısını (ızgara halinde) ekrana yazdırır. |\n",
    "\n",
    "---\n",
    "\n",
    "##  Performans Değerlendirmesi\n",
    "\n",
    "Ajanlar, aynı harita üzerinde çalıştırılarak **ortalama performans**, **toplanan kaynak sayısı** ve **maruz kalınan tehlike sayısı** açısından kıyaslanır.\n",
    "\n",
    "| Metrik | Agent1 | Agent2 | Agent3 |\n",
    "|--------|---------|---------|---------|\n",
    "| Toplanan Kaynak | Orta | Yüksek | Düşük |\n",
    "| Tehlike Çarpışması | Orta | Düşük | Yüksek |\n",
    "| Ortalama Skor | Orta | Yüksek | Düşük |\n",
    "\n",
    "---\n",
    "\n",
    "##  Sonuç\n",
    "\n",
    "Bu çalışma, **model tabanlı refleks ajanların bellek, karar verme ve rastgelelik seviyelerine göre performans farklarını** göstermektedir.  \n",
    "Sonuç olarak:\n",
    "\n",
    "- **Agent2**, dengeli stratejisiyle en başarılı ajandır.  \n",
    "- **Agent1**, sınırlı hafızasıyla orta düzeyde performans sergiler.  \n",
    "- **Agent3**, karar zekâsı olmadan rastgele hareket ettiği için düşük performans gösterir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e33423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simülasyon başlatılıyor...\n",
      "\n",
      "Deneme 1: -93 puan\n",
      "Deneme 2: 7 puan\n",
      "Deneme 3: 7 puan\n",
      "Deneme 4: -93 puan\n",
      "Deneme 5: 7 puan\n",
      "\n",
      "========================================\n",
      "Ortalama Performans: -33.00 puan\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# AGENT1 - Temel Model-Tabanlı Refleks Ajan\n",
    "\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell\n",
    "import random\n",
    "import statistics\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "\n",
    "class Direction(Enum):\n",
    "    \"\"\"Yön tanımları\"\"\"\n",
    "    NORTH = 0\n",
    "    EAST = 1\n",
    "    SOUTH = 2\n",
    "    WEST = 3\n",
    "\n",
    "# Her yön için hareket deltaları\n",
    "move_delta = {\n",
    "    Direction.NORTH: (-1, 0),\n",
    "    Direction.SOUTH: (1, 0),\n",
    "    Direction.EAST: (0, 1),\n",
    "    Direction.WEST: (0, -1)\n",
    "}\n",
    "\n",
    "class XYEnvironment:\n",
    "    \"\"\"\n",
    "    10x10 grid ortamı.\n",
    "    - 'E': Engel (geçilemez)\n",
    "    - 'Y': Yumurta (+2 puan)\n",
    "    - 'S': Süt (+5 puan)\n",
    "    - 'B': Bataklık (-10 puan)\n",
    "    - 'A': Ayı (-100 puan)\n",
    "    - 'C': Başlangıç pozisyonu\n",
    "    - ' ': Boş alan\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid, start):\n",
    "        self.grid = [row[:] for row in grid]  # Deep copy\n",
    "        self.start = start\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "    \n",
    "    def in_bounds(self, pos):\n",
    "        \"\"\"Pozisyon sınırlar içinde mi?\"\"\"\n",
    "        r, c = pos\n",
    "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
    "    \n",
    "    def is_obstacle(self, pos):\n",
    "        \"\"\"Pozisyon engel mi?\"\"\"\n",
    "        r, c = pos\n",
    "        return self.grid[r][c] == \"E\"\n",
    "    \n",
    "    def percept(self, pos):\n",
    "        \"\"\"\n",
    "        Kısmi gözlem: Bulunulan hücre + 4 komşu hücrenin içeriği\n",
    "        \"\"\"\n",
    "        r, c = pos\n",
    "        obs = {}\n",
    "        obs['current'] = self.grid[r][c]\n",
    "        \n",
    "        neighbours = {}\n",
    "        for d, delta in move_delta.items():\n",
    "            nr, nc = r + delta[0], c + delta[1]\n",
    "            if self.in_bounds((nr, nc)):\n",
    "                neighbours[d.name] = self.grid[nr][nc]\n",
    "            else:\n",
    "                neighbours[d.name] = None\n",
    "        obs['neighbours'] = neighbours\n",
    "        return obs\n",
    "    \n",
    "    def execute(self, agent, action):\n",
    "        \"\"\"\n",
    "        Aksiyonu uygula ve performans değişimini döndür.\n",
    "        Aksiyonlar: 'FORWARD', 'LEFT', 'RIGHT', 'LOAD'\n",
    "        \"\"\"\n",
    "        perf_delta = 0\n",
    "        \n",
    "        if action == \"LEFT\":\n",
    "            # Sadece sola dön\n",
    "            agent.orientation = Direction((agent.orientation.value - 1) % 4)\n",
    "            \n",
    "        elif action == \"RIGHT\":\n",
    "            # Sadece sağa dön\n",
    "            agent.orientation = Direction((agent.orientation.value + 1) % 4)\n",
    "            \n",
    "        elif action == \"FORWARD\":\n",
    "            # İleri git\n",
    "            dr, dc = move_delta[agent.orientation]\n",
    "            newpos = (agent.pos[0] + dr, agent.pos[1] + dc)\n",
    "            \n",
    "            if not self.in_bounds(newpos) or self.is_obstacle(newpos):\n",
    "                # Engele veya sınır dışına gidilemez, yerinde kal\n",
    "                return perf_delta\n",
    "            \n",
    "            agent.pos = newpos\n",
    "            \n",
    "            # Yeni hücredeki tehlikeleri kontrol et\n",
    "            r, c = agent.pos\n",
    "            cell_content = self.grid[r][c]\n",
    "            if cell_content == \"B\":\n",
    "                perf_delta -= 10  # Bataklık\n",
    "            elif cell_content == \"A\":\n",
    "                perf_delta -= 100  # Ayı\n",
    "                \n",
    "        elif action == \"LOAD\":\n",
    "            # Bulunulan hücredeki kaynağı yükle\n",
    "            r, c = agent.pos\n",
    "            content = self.grid[r][c]\n",
    "            \n",
    "            if content == \"Y\" and not agent.carry_egg:\n",
    "                agent.carry_egg = True\n",
    "                self.grid[r][c] = \" \"  # Yumurtayı haritadan kaldır\n",
    "                perf_delta += 2\n",
    "            elif content == \"S\" and not agent.carry_milk:\n",
    "                agent.carry_milk = True\n",
    "                self.grid[r][c] = \" \"  # Sütü haritadan kaldır\n",
    "                perf_delta += 5\n",
    "            \n",
    "            # LOAD sonrası hücredeki tehlikeleri kontrol et\n",
    "            cell_content = self.grid[r][c]\n",
    "            if cell_content == \"B\":\n",
    "                perf_delta -= 10\n",
    "            elif cell_content == \"A\":\n",
    "                perf_delta -= 100\n",
    "        \n",
    "        return perf_delta\n",
    "\n",
    "\n",
    "class Agent:\n",
    "  \n",
    "    \n",
    "    def __init__(self, start, orientation=Direction.EAST):\n",
    "        self.pos = start\n",
    "        self.orientation = orientation\n",
    "        self.carry_egg = False\n",
    "        self.carry_milk = False\n",
    "        self.memory = {}  # Gözlemlenen hücrelerin hafızası\n",
    "    \n",
    "    def perceive(self, env):\n",
    "        \"\"\"Ortamdan algılama yap ve hafızaya kaydet\"\"\"\n",
    "        p = env.percept(self.pos)\n",
    "        \n",
    "        # Bulunulan hücreyi hafızaya kaydet\n",
    "        r, c = self.pos\n",
    "        self.memory[(r, c)] = p['current']\n",
    "        \n",
    "        # Komşuları hafızaya kaydet\n",
    "        for dname, content in p['neighbours'].items():\n",
    "            if content is not None:\n",
    "                dir_enum = Direction[dname]\n",
    "                dr, dc = move_delta[dir_enum]\n",
    "                nr, nc = r + dr, c + dc\n",
    "                self.memory[(nr, nc)] = content\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def program(self, percept, env):\n",
    "      \n",
    "        cur = percept['current']\n",
    "        \n",
    "        # 1. Bulunduğu yerde kaynak varsa yükle\n",
    "        if cur == \"S\" and not self.carry_milk:\n",
    "            return \"LOAD\"\n",
    "        if cur == \"Y\" and not self.carry_egg:\n",
    "            return \"LOAD\"\n",
    "        \n",
    "        # 2. Bilinen kaynaklara yönel\n",
    "        targets = []\n",
    "        for (pos, content) in self.memory.items():\n",
    "            if content == \"S\" and not self.carry_milk:\n",
    "                targets.append(pos)\n",
    "            elif content == \"Y\" and not self.carry_egg:\n",
    "                targets.append(pos)\n",
    "        \n",
    "        if targets:\n",
    "            # En yakın hedefi seç (Manhattan distance)\n",
    "            target = min(targets, key=lambda t: abs(t[0] - self.pos[0]) + abs(t[1] - self.pos[1]))\n",
    "            \n",
    "            # Hedefe doğru yönelim belirle\n",
    "            dr = target[0] - self.pos[0]\n",
    "            dc = target[1] - self.pos[1]\n",
    "            \n",
    "            # Önce dikey, sonra yatay hareket tercih et\n",
    "            if dr < 0 and self.orientation != Direction.NORTH:\n",
    "                return self.turn_towards(Direction.NORTH)\n",
    "            elif dr > 0 and self.orientation != Direction.SOUTH:\n",
    "                return self.turn_towards(Direction.SOUTH)\n",
    "            elif dc < 0 and self.orientation != Direction.WEST:\n",
    "                return self.turn_towards(Direction.WEST)\n",
    "            elif dc > 0 and self.orientation != Direction.EAST:\n",
    "                return self.turn_towards(Direction.EAST)\n",
    "            else:\n",
    "                # Doğru yöne bakıyoruz, ileri git\n",
    "                return \"FORWARD\"\n",
    "        \n",
    "        # 3. Keşif: İleri gidebiliyorsa %70 ileri git, yoksa dön\n",
    "        dr, dc = move_delta[self.orientation]\n",
    "        forward_pos = (self.pos[0] + dr, self.pos[1] + dc)\n",
    "        \n",
    "        if env.in_bounds(forward_pos) and not env.is_obstacle(forward_pos):\n",
    "            if random.random() < 0.7:\n",
    "                return \"FORWARD\"\n",
    "        \n",
    "        # Rastgele dön\n",
    "        return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "    \n",
    "    def turn_towards(self, desired_dir):\n",
    "        \"\"\"İstenilen yöne dönmek için LEFT veya RIGHT döndür\"\"\"\n",
    "        diff = (desired_dir.value - self.orientation.value) % 4\n",
    "        \n",
    "        if diff == 0:\n",
    "            return \"FORWARD\"\n",
    "        elif diff == 1:\n",
    "            return \"RIGHT\"\n",
    "        elif diff == 3:\n",
    "            return \"LEFT\"\n",
    "        else:  # diff == 2 (ters yön)\n",
    "            return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_grid():\n",
    "    \"\"\"10x10 grid haritası oluştur\"\"\"\n",
    "    # Tüm kenarlar engel ile başla\n",
    "    grid = [[\"E\"] * 10 for _ in range(10)]\n",
    "    \n",
    "    # İç kısmı boşalt\n",
    "    for i in range(1, 9):\n",
    "        for j in range(1, 9):\n",
    "            grid[i][j] = \" \"\n",
    "    \n",
    "    # Kaynakları ve tehlikeleri yerleştir\n",
    "    placements = {\n",
    "        (1, 2): \"Y\", (2, 2): \"Y\", (2, 3): \"Y\",  # Yumurtalar\n",
    "        (1, 6): \"S\", (2, 6): \"S\", (5, 1): \"S\",  # Sütler\n",
    "        (4, 6): \"Y\", (4, 7): \"Y\",                # Daha fazla yumurta\n",
    "        (7, 5): \"B\", (6, 5): \"B\",                # Bataklıklar\n",
    "        (8, 2): \"A\"                               # Ayı\n",
    "    }\n",
    "    \n",
    "    for (r, c), value in placements.items():\n",
    "        grid[r][c] = value\n",
    "    \n",
    "    # Başlangıç pozisyonu\n",
    "    start = (3, 1)\n",
    "    grid[start[0]][start[1]] = \"C\"\n",
    "    \n",
    "    return grid, start\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_episode(grid, start, max_steps=50):\n",
    "    \"\"\"Bir epizodu çalıştır ve toplam performansı döndür\"\"\"\n",
    "    env = XYEnvironment(grid, start)\n",
    "    agent = Agent(start, orientation=Direction.EAST)\n",
    "    total_perf = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        percept = agent.perceive(env)\n",
    "        action = agent.program(percept, env)\n",
    "        perf_delta = env.execute(agent, action)\n",
    "        total_perf += perf_delta\n",
    "    \n",
    "    return total_perf\n",
    "\n",
    "\n",
    "\n",
    "GRID, START = create_grid()\n",
    "performances = []\n",
    "\n",
    "print(\"Simülasyon başlatılıyor...\\n\")\n",
    "for i in range(5):\n",
    "    perf = run_episode(GRID, START, max_steps=50)\n",
    "    performances.append(perf)\n",
    "    print(f\"Deneme {i+1}: {perf} puan\")\n",
    "\n",
    "mean_perf = statistics.mean(performances)\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"Ortalama Performans: {mean_perf:.2f} puan\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd446462",
   "metadata": {},
   "source": [
    "## AGENT1 : \n",
    "\n",
    "###  Ajanın Özellikleri\n",
    "\n",
    "1. **Model-Based Reflex Agent (Model Tabanlı Refleks Ajan)**  \n",
    "   - Ajan, ortamdan aldığı algıları (percept) hafızasında saklayarak model oluşturur.  \n",
    "   - Önceden gözlemlenen hücrelerin içeriğini (`self.memory`) kaydeder ve kararlarında bu bilgiyi kullanır.\n",
    "\n",
    "2. **Kısmi Gözlem (Partial Observability) ile Çalışma**  \n",
    "   - Ajan sadece bulunduğu hücreyi ve dört komşu hücreyi gözlemleyebilir.  \n",
    "   - Bu, gerçekçi bir kısıt getirerek ajanı belirsizlik altında karar vermeye zorlar.\n",
    "\n",
    "3. **Hedef Odaklı Karar Verme**  \n",
    "   - Hafızasında bulunan “Süt (S)” veya “Yumurta (Y)” hücrelerine yönelir.  \n",
    "   - Eğer bilinen bir kaynak yoksa, rastgele keşif stratejisi uygular.\n",
    "\n",
    "4. **Eylem Kümesi**  \n",
    "   - `\"FORWARD\"`, `\"LEFT\"`, `\"RIGHT\"`, `\"LOAD\"` aksiyonlarını kullanır.  \n",
    "   - `\"LOAD\"` komutu, ajan bulunduğu hücredeki kaynağı topladığında performans artışı sağlar.\n",
    "\n",
    "5. **Performans Ölçütü (Performance Function)**  \n",
    "   - Pozitif ödüller:  \n",
    "     - Yumurta toplama: **+2 puan**  \n",
    "     - Süt toplama: **+5 puan**  \n",
    "   - Negatif ödüller:  \n",
    "     - Bataklığa girme: **-10 puan**  \n",
    "     - Ayı hücresine girme: **-100 puan**\n",
    "\n",
    "6. **Basit Yönelim Sistemi**  \n",
    "   - Ajan, bulunduğu yönü `Direction` enum sınıfı ile temsil eder ve sola/sağa dönerek yönünü değiştirir.  \n",
    "   - Hedef hücreye göre yönünü ayarlamak için `turn_towards()` fonksiyonunu kullanır.\n",
    "\n",
    "---\n",
    "\n",
    "###  Geliştirilebilecek Yönler\n",
    "\n",
    "1. **Hedef Planlama ve Yol Bulma (Path Planning)**  \n",
    "   - Şu anda ajan sadece Manhattan mesafesine göre en yakın hedefe yöneliyor.  \n",
    "   - Bunun yerine **BFS**, **A\\*** veya **Dijkstra** gibi algoritmalarla engelleri ve tehlikeleri dikkate alan bir yol planlama stratejisi eklenebilir.\n",
    "\n",
    "2. **Bellek Güncelleme Mekanizması**  \n",
    "   - Ajanın hafızası sadece gözlemlenen hücreleri kaydediyor, ancak ortamda değişim (örneğin kaynakların toplanması) sonrası bellek güncellenmiyor.  \n",
    "   - “Artık boş” olan hücrelerin hafızadan silinmesi veya güncellenmesi daha verimli olur.\n",
    "\n",
    "3. **Risk Değerlendirme (Utility-Based Decision)**  \n",
    "   - Karar mekanizmasında sadece kaynak odaklı hareket var.  \n",
    "   - Bunun yerine **beklenen kazanç (expected utility)** hesaplayan bir sistem eklenebilir:  \n",
    "     örn. `utility = reward - risk`.\n",
    "\n",
    "4. **Enerji veya Zaman Sınırlaması**  \n",
    "   - Şu an ajan sınırsız hareket ediyor.  \n",
    "   - Bir “enerji” veya “adım limiti” eklenerek stratejik optimizasyon teşvik edilebilir.\n",
    "\n",
    "5. **Öğrenme Yeteneği (Learning Agent)**  \n",
    "   - Performans geçmişine göre aksiyonlarını optimize eden bir öğrenme mekanizması (örneğin Q-Learning veya SARSA) eklenebilir.  \n",
    "   - Bu sayede ajan zamanla daha verimli yollar keşfedebilir.\n",
    "\n",
    "6. **Çoklu Hedef Önceliklendirme**  \n",
    "   - Şu anda süt ve yumurta aynı öneme sahip.  \n",
    "   - Farklı kaynak türlerine ağırlık atayan bir **öncelik sistemi** geliştirilebilir.\n",
    "\n",
    "7. **Bataklık ve Ayıdan Kaçınma**  \n",
    "   - Ajan, tehlikeli hücreleri bilse de bunlardan kaçınma stratejisine sahip değil.  \n",
    "   - Bellekte “tehlikeli bölgeler” olarak işaretleme ve rotadan çıkarma (avoidance logic) eklenebilir.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0884653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "HARITA (10x10)\n",
      "==================================================\n",
      "Satır 0: EEEEEEEEEE\n",
      "Satır 1: E Y   S  E\n",
      "Satır 2: E YY  S  E\n",
      "Satır 3: EC       E\n",
      "Satır 4: E     YY E\n",
      "Satır 5: E S      E\n",
      "Satır 6: E    B   E\n",
      "Satır 7: E    B   E\n",
      "Satır 8: E A      E\n",
      "Satır 9: EEEEEEEEEE\n",
      "\n",
      "Legend: E=Engel, Y=Yumurta(+2), S=Süt(+5), B=Bataklık(-10), A=Ayı(-100), C=Başlangıç\n",
      "==================================================\n",
      "\n",
      "🚜 Simülasyon başlatılıyor (5 x 50 adım)...\n",
      "\n",
      "📊 DETAYLI DENEME #1:\n",
      "--------------------------------------------------\n",
      "Başlangıç: (3, 1), Yön: EAST\n",
      "Adım 1: FORWARD -> Pozisyon: (3, 2)\n",
      "Adım 2: LEFT -> Pozisyon: (3, 2), Yön: NORTH, Puan: +0 (Toplam: 0)\n",
      "Adım 3: FORWARD -> Pozisyon: (2, 2)\n",
      "Adım 4: LOAD -> Pozisyon: (2, 2), Yön: NORTH, Puan: +2 (Toplam: 2)\n",
      "Adım 5: FORWARD -> Pozisyon: (1, 2)\n",
      "Adım 6: LEFT -> Pozisyon: (1, 2), Yön: WEST, Puan: +0 (Toplam: 2)\n",
      "Adım 7: FORWARD -> Pozisyon: (1, 1)\n",
      "Adım 8: LEFT -> Pozisyon: (1, 1), Yön: SOUTH, Puan: +0 (Toplam: 2)\n",
      "Adım 9: FORWARD -> Pozisyon: (2, 1)\n",
      "Adım 10: FORWARD -> Pozisyon: (3, 1)\n",
      "Adım 11: FORWARD -> Pozisyon: (4, 1)\n",
      "Adım 12: FORWARD -> Pozisyon: (5, 1)\n",
      "Adım 13: LEFT -> Pozisyon: (5, 1), Yön: EAST, Puan: +0 (Toplam: 2)\n",
      "Adım 14: FORWARD -> Pozisyon: (5, 2)\n",
      "Adım 15: LOAD -> Pozisyon: (5, 2), Yön: EAST, Puan: +5 (Toplam: 7)\n",
      "Adım 16: FORWARD -> Pozisyon: (5, 3)\n",
      "Adım 17: FORWARD -> Pozisyon: (5, 4)\n",
      "Adım 18: FORWARD -> Pozisyon: (5, 5)\n",
      "Adım 19: FORWARD -> Pozisyon: (5, 6)\n",
      "Adım 20: FORWARD -> Pozisyon: (5, 7)\n",
      "Adım 21: FORWARD -> Pozisyon: (5, 8)\n",
      "Adım 22: RIGHT -> Pozisyon: (5, 8), Yön: SOUTH, Puan: +0 (Toplam: 7)\n",
      "Adım 23: FORWARD -> Pozisyon: (6, 8)\n",
      "Adım 24: FORWARD -> Pozisyon: (7, 8)\n",
      "Adım 25: FORWARD -> Pozisyon: (8, 8)\n",
      "Adım 26: LEFT -> Pozisyon: (8, 8), Yön: EAST, Puan: +0 (Toplam: 7)\n",
      "Adım 27: LEFT -> Pozisyon: (8, 8), Yön: NORTH, Puan: +0 (Toplam: 7)\n",
      "Adım 28: FORWARD -> Pozisyon: (7, 8)\n",
      "Adım 29: LEFT -> Pozisyon: (7, 8), Yön: WEST, Puan: +0 (Toplam: 7)\n",
      "Adım 30: FORWARD -> Pozisyon: (7, 7)\n",
      "Adım 31: FORWARD -> Pozisyon: (7, 6)\n",
      "Adım 32: LEFT -> Pozisyon: (7, 6), Yön: SOUTH, Puan: +0 (Toplam: 7)\n",
      "Adım 33: FORWARD -> Pozisyon: (8, 6)\n",
      "Adım 34: LEFT -> Pozisyon: (8, 6), Yön: EAST, Puan: +0 (Toplam: 7)\n",
      "Adım 35: FORWARD -> Pozisyon: (8, 7)\n",
      "Adım 36: RIGHT -> Pozisyon: (8, 7), Yön: SOUTH, Puan: +0 (Toplam: 7)\n",
      "Adım 37: LEFT -> Pozisyon: (8, 7), Yön: EAST, Puan: +0 (Toplam: 7)\n",
      "Adım 38: FORWARD -> Pozisyon: (8, 8)\n",
      "Adım 39: LEFT -> Pozisyon: (8, 8), Yön: NORTH, Puan: +0 (Toplam: 7)\n",
      "Adım 40: FORWARD -> Pozisyon: (7, 8)\n",
      "Adım 41: LEFT -> Pozisyon: (7, 8), Yön: WEST, Puan: +0 (Toplam: 7)\n",
      "Adım 42: FORWARD -> Pozisyon: (7, 7)\n",
      "Adım 43: FORWARD -> Pozisyon: (7, 6)\n",
      "Adım 44: RIGHT -> Pozisyon: (7, 6), Yön: NORTH, Puan: +0 (Toplam: 7)\n",
      "Adım 45: FORWARD -> Pozisyon: (6, 6)\n",
      "Adım 46: RIGHT -> Pozisyon: (6, 6), Yön: EAST, Puan: +0 (Toplam: 7)\n",
      "Adım 47: FORWARD -> Pozisyon: (6, 7)\n",
      "Adım 48: FORWARD -> Pozisyon: (6, 8)\n",
      "Adım 49: RIGHT -> Pozisyon: (6, 8), Yön: SOUTH, Puan: +0 (Toplam: 7)\n",
      "Adım 50: LEFT -> Pozisyon: (6, 8), Yön: EAST, Puan: +0 (Toplam: 7)\n",
      "\n",
      "Son Pozisyon: (6, 8), Son Yön: EAST\n",
      "Yumurta taşıyor: True, Süt taşıyor: True\n",
      "Hafızadaki tehlike sayısı: 2\n",
      "\n",
      "✅ Deneme 1 Sonucu: 7 puan\n",
      "\n",
      "Deneme 2: 7 puan\n",
      "Deneme 3: 7 puan\n",
      "Deneme 4: 7 puan\n",
      "Deneme 5: 7 puan\n",
      "\n",
      "==================================================\n",
      "📈 PERFORMANS ÖZETİ\n",
      "==================================================\n",
      "Performans Değerleri (Liste): [7, 7, 7, 7, 7]\n",
      "Ortalama Performans (5 Deneme): 7.00 puan\n",
      "En İyi Performans: 7 puan\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#AGENT2 - Risk Odaklı Model-Tabanlı Refleks Ajan\n",
    "\n",
    "\n",
    "import random\n",
    "import statistics\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Direction(Enum):\n",
    "    \"\"\"Yön tanımları: Kuzey=0, Doğu=1, Güney=2, Batı=3\"\"\"\n",
    "    NORTH = 0\n",
    "    EAST = 1\n",
    "    SOUTH = 2\n",
    "    WEST = 3\n",
    "\n",
    "# Her yön için hareket deltaları (row, col)\n",
    "move_delta = {\n",
    "    Direction.NORTH: (-1, 0),\n",
    "    Direction.SOUTH: (1, 0),\n",
    "    Direction.EAST: (0, 1),\n",
    "    Direction.WEST: (0, -1)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class XYEnvironment:\n",
    "    \"\"\"\n",
    "    10x10 grid ortamı.\n",
    "    - 'E': Engel (geçilemez)\n",
    "    - 'Y': Yumurta (+2 puan)\n",
    "    - 'S': Süt (+5 puan)\n",
    "    - 'B': Bataklık (-10 puan) -> Performans kaybı\n",
    "    - 'A': Ayı (-100 puan) -> Performans kaybı\n",
    "    - 'C': Başlangıç pozisyonu\n",
    "    - ' ': Boş alan\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, grid, start):\n",
    "        # Ortamın başlangıç durumunu sakla (simülasyon tekrarı için)\n",
    "        self._initial_grid = [row[:] for row in grid]\n",
    "        self.start = start\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Ortamı başlangıç durumuna döndürür.\"\"\"\n",
    "        self.grid = [row[:] for row in self._initial_grid]\n",
    "    \n",
    "    def in_bounds(self, pos):\n",
    "        \"\"\"Pozisyon sınırlar içinde mi?\"\"\"\n",
    "        r, c = pos\n",
    "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
    "    \n",
    "    def is_obstacle(self, pos):\n",
    "        \"\"\"Pozisyon engel mi?\"\"\"\n",
    "        r, c = pos\n",
    "        return self.grid[r][c] == \"E\"\n",
    "    \n",
    "    def percept(self, pos):\n",
    "        \"\"\"\n",
    "        Kısmi gözlem: Bulunulan hücre + 4 komşu hücrenin içeriği\n",
    "        \"\"\"\n",
    "        r, c = pos\n",
    "        obs = {}\n",
    "        # Bulunulan hücrenin içeriği\n",
    "        obs['current'] = self.grid[r][c]\n",
    "        \n",
    "        # Komşu hücrelerin içeriği\n",
    "        neighbours = {}\n",
    "        for d, delta in move_delta.items():\n",
    "            nr, nc = r + delta[0], c + delta[1]\n",
    "            if self.in_bounds((nr, nc)):\n",
    "                neighbours[d.name] = self.grid[nr][nc]\n",
    "            else:\n",
    "                neighbours[d.name] = \"E\" # Sınır dışını engel olarak algıla\n",
    "        obs['neighbours'] = neighbours\n",
    "        return obs\n",
    "    \n",
    "    def execute(self, agent, action):\n",
    "        \"\"\"\n",
    "        Aksiyonu uygula ve performans değişimini döndür.\n",
    "    \n",
    "        \"\"\"\n",
    "        perf_delta = 0\n",
    "        \n",
    "        # 1. Yön Değiştirme (LEFT/RIGHT)\n",
    "        if action == \"LEFT\":\n",
    "            agent.orientation = Direction((agent.orientation.value - 1) % 4)\n",
    "            \n",
    "        elif action == \"RIGHT\":\n",
    "            agent.orientation = Direction((agent.orientation.value + 1) % 4)\n",
    "            \n",
    "        # 2. İlerleme (FORWARD)\n",
    "        elif action == \"FORWARD\":\n",
    "            dr, dc = move_delta[agent.orientation]\n",
    "            newpos = (agent.pos[0] + dr, agent.pos[1] + dc)\n",
    "            \n",
    "            if not self.in_bounds(newpos) or self.is_obstacle(newpos):\n",
    "                # Engele veya sınır dışına gidilemez, yerinde kal\n",
    "                return perf_delta\n",
    "            \n",
    "            agent.pos = newpos\n",
    "            \n",
    "            # Yeni hücredeki tehlikeleri kontrol et (Ek hareket yapmadan performans kaybı)\n",
    "            r, c = agent.pos\n",
    "            cell_content = self.grid[r][c]\n",
    "            if cell_content == \"B\":\n",
    "                perf_delta -= 10  # Bataklık\n",
    "            elif cell_content == \"A\":\n",
    "                perf_delta -= 100  # Ayı\n",
    "                \n",
    "        # 3. Yükleme (LOAD)\n",
    "        elif action == \"LOAD\":\n",
    "            # Bulunulan hücredeki kaynağı yükle\n",
    "            r, c = agent.pos\n",
    "            content = self.grid[r][c]\n",
    "            \n",
    "            if content == \"Y\" and not agent.carry_egg:\n",
    "                agent.carry_egg = True\n",
    "                self.grid[r][c] = \" \"  # Yumurtayı haritadan kaldır\n",
    "                perf_delta += 2\n",
    "            elif content == \"S\" and not agent.carry_milk:\n",
    "                agent.carry_milk = True\n",
    "                self.grid[r][c] = \" \"  # Sütü haritadan kaldır\n",
    "                perf_delta += 5\n",
    "        \n",
    "        return perf_delta\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Model-based reflex agent: Hafıza ve tehlike bilgisi kullanarak karar verir.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, start, orientation=Direction.EAST):\n",
    "        self.start = start\n",
    "        self.initial_orientation = orientation\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Ajanı başlangıç durumuna döndürür.\"\"\"\n",
    "        self.pos = self.start\n",
    "        self.orientation = self.initial_orientation\n",
    "        self.carry_egg = False\n",
    "        self.carry_milk = False\n",
    "        self.memory = {}  # Gözlemlenen hücrelerin içeriği\n",
    "        self.dangers = set()  # Tehlikeli hücreler (ayı ve bataklık)\n",
    "        self.visited = set()  # Ziyaret edilen hücreler\n",
    "        self.visited.add(self.start)\n",
    "    \n",
    "    def perceive(self, env):\n",
    "        \"\"\"Ortamdan algılama yap ve hafızaya kaydet\"\"\"\n",
    "        p = env.percept(self.pos)\n",
    "        \n",
    "        # Bulunulan hücreyi hafızaya kaydet/güncelle\n",
    "        r, c = self.pos\n",
    "        self.memory[(r, c)] = p['current']\n",
    "        \n",
    "        # Komşuları hafızaya kaydet/güncelle\n",
    "        for dname, content in p['neighbours'].items():\n",
    "            if content is not None:\n",
    "                dir_enum = Direction[dname]\n",
    "                dr, dc = move_delta[dir_enum]\n",
    "                nr, nc = r + dr, c + dc\n",
    "                \n",
    "                # Sınır dışı/Engel değilse (None veya \"E\" değilse)\n",
    "                if env.in_bounds((nr, nc)) and not env.is_obstacle((nr, nc)):\n",
    "                    self.memory[(nr, nc)] = content\n",
    "                \n",
    "                # Tehlikeleri işaretle\n",
    "                if content in ('A', 'B'):\n",
    "                    self.dangers.add((nr, nc))\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def is_safe(self, pos):\n",
    "        \n",
    "        return pos not in self.dangers\n",
    "    \n",
    "    def program(self, percept, env):\n",
    "       \n",
    "        cur = percept['current']\n",
    "        \n",
    "        # 1. Amaç: Bulunduğu yerde kaynak varsa yükle\n",
    "        if (cur == \"S\" and not self.carry_milk) or \\\n",
    "           (cur == \"Y\" and not self.carry_egg):\n",
    "            return \"LOAD\"\n",
    "        \n",
    "        # Mevcut yön:\n",
    "        dr, dc = move_delta[self.orientation]\n",
    "        forward_pos = (self.pos[0] + dr, self.pos[1] + dc)\n",
    "        \n",
    "        # 2. Tehlike Kontrolü: İlerideki hücre tehlikeli, engel veya sınır dışı mı?\n",
    "        forward_safe = env.in_bounds(forward_pos) and \\\n",
    "                       not env.is_obstacle(forward_pos) and \\\n",
    "                       self.is_safe(forward_pos)\n",
    "        \n",
    "        if not forward_safe:\n",
    "            # İleride engel, sınır veya tehlike var, dön\n",
    "            return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "            \n",
    "        # 3. Amaç: Güvenli kaynaklara yönel\n",
    "        targets = []\n",
    "        for pos, content in self.memory.items():\n",
    "            if pos in self.dangers: continue # Güvenli olmayan hedefleri atla\n",
    "                \n",
    "            if content == \"S\" and not self.carry_milk:\n",
    "                targets.append((pos, 5)) # Süt öncelik 5\n",
    "            elif content == \"Y\" and not self.carry_egg:\n",
    "                targets.append((pos, 2)) # Yumurta öncelik 2\n",
    "                \n",
    "        if targets:\n",
    "          \n",
    "            target_pos = max(targets, key=lambda t: (\n",
    "                t[1], # Önce değer\n",
    "                - (abs(t[0][0] - self.pos[0]) + abs(t[0][1] - self.pos[1])) # Sonra yakınlık\n",
    "            ))[0]\n",
    "            \n",
    "            # Hedefe doğru yönelim belirle\n",
    "            action = self.get_action_to_target(target_pos)\n",
    "            \n",
    "            # Eğer aksiyon ileri gitmekse ve güvenliyse uygula\n",
    "            if action == \"FORWARD\":\n",
    "                return \"FORWARD\"\n",
    "            # Dönme eylemi ise, dön ve yeni ileri pozisyonu kontrol et\n",
    "            elif action in (\"LEFT\", \"RIGHT\"):\n",
    "                return action\n",
    "            # Hiçbir şey yapılamıyorsa (hedef yanımızda ama ileri gidemiyoruz)\n",
    "            else:\n",
    "                return random.choice([\"LEFT\", \"RIGHT\"]) # Rastgele dön\n",
    "        \n",
    "        # 4. Keşif: Güvenli ve ziyaret edilmemiş hücrelere doğru ilerle\n",
    "        if forward_safe:\n",
    "            if forward_pos not in self.visited:\n",
    "                self.visited.add(forward_pos)\n",
    "                return \"FORWARD\"\n",
    "            # Ziyaret edilmiş güvenli bir yolda %50 şansla ilerle (döngüden kaçınma)\n",
    "            elif random.random() < 0.5:\n",
    "                return \"FORWARD\"\n",
    "                \n",
    "        # 5. Son çare: Rastgele güvenli bir yöne dön\n",
    "        return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "    \n",
    "    def get_action_to_target(self, target_pos):\n",
    "        \"\"\"Hedefe doğru dönmek için gerekli eylemi bulur.\"\"\"\n",
    "        dr = target_pos[0] - self.pos[0]\n",
    "        dc = target_pos[1] - self.pos[1]\n",
    "        \n",
    "        # Öncelikli hareket yönünü belirle\n",
    "        desired_dir = None\n",
    "        if abs(dr) > abs(dc): # Dikey hareket öncelikli\n",
    "            desired_dir = Direction.NORTH if dr < 0 else Direction.SOUTH\n",
    "        elif abs(dc) > 0: # Yatay hareket öncelikli (dr=0 olabilir)\n",
    "            desired_dir = Direction.WEST if dc < 0 else Direction.EAST\n",
    "        else: # Zaten hedefteyiz (LOAD'dan sonra buraya düşebiliriz)\n",
    "            return \"LOAD\" \n",
    "            \n",
    "        # Mevcut yönden hedef yöne dönme eylemini hesapla\n",
    "        if desired_dir is not None:\n",
    "            diff = (desired_dir.value - self.orientation.value) % 4\n",
    "            \n",
    "            if diff == 0:\n",
    "                return \"FORWARD\"\n",
    "            elif diff == 1:\n",
    "                return \"RIGHT\"\n",
    "            elif diff == 3:\n",
    "                return \"LEFT\"\n",
    "            else: # diff == 2 (Ters yön)\n",
    "                return random.choice([\"LEFT\", \"RIGHT\"])\n",
    "        \n",
    "        return random.choice([\"LEFT\", \"RIGHT\"]) # Güvenlik\n",
    "\n",
    "\n",
    "\n",
    "def create_grid():\n",
    "    \"\"\"10x10 grid haritası oluştur\"\"\"\n",
    "    # ... (Aynı harita tanımı) ...\n",
    "    grid = [[\"E\"] * 10 for _ in range(10)]\n",
    "    \n",
    "    for i in range(1, 9):\n",
    "        for j in range(1, 9):\n",
    "            grid[i][j] = \" \"\n",
    "    \n",
    "    placements = {\n",
    "        (1, 2): \"Y\", (2, 2): \"Y\", (2, 3): \"Y\", \n",
    "        (1, 6): \"S\", (2, 6): \"S\", \n",
    "        (5, 2): \"S\", \n",
    "        (4, 6): \"Y\", (4, 7): \"Y\", \n",
    "        (7, 5): \"B\", (6, 5): \"B\", \n",
    "        (8, 2): \"A\" \n",
    "    }\n",
    "    \n",
    "    for (r, c), value in placements.items():\n",
    "        grid[r][c] = value\n",
    "    \n",
    "    start = (3, 1)\n",
    "    grid[start[0]][start[1]] = \"C\"\n",
    "    \n",
    "    return grid, start\n",
    "\n",
    "def run_episode(grid, start, max_steps=50, verbose=False):\n",
    "    \"\"\"Bir epizodu çalıştır ve toplam performansı döndür\"\"\"\n",
    "    env = XYEnvironment(grid, start)\n",
    "    agent = Agent(start, orientation=Direction.EAST)\n",
    "    total_perf = 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Başlangıç: {start}, Yön: {agent.orientation.name}\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        percept = agent.perceive(env)\n",
    "        action = agent.program(percept, env)\n",
    "        perf_delta = env.execute(agent, action)\n",
    "        total_perf += perf_delta\n",
    "        \n",
    "        if verbose:\n",
    "            # Sadece LOAD, FORWARD (Bat/Ayı) veya yön değiştirmeler için çıktı ver\n",
    "            if action == \"LOAD\" or (action == \"FORWARD\" and perf_delta != 0) or action in (\"LEFT\", \"RIGHT\"):\n",
    "                print(f\"Adım {step+1}: {action} -> Pozisyon: {agent.pos}, Yön: {agent.orientation.name}, Puan: {perf_delta:+d} (Toplam: {total_perf})\")\n",
    "            elif action == \"FORWARD\":\n",
    "                 print(f\"Adım {step+1}: {action} -> Pozisyon: {agent.pos}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nSon Pozisyon: {agent.pos}, Son Yön: {agent.orientation.name}\")\n",
    "        print(f\"Yumurta taşıyor: {agent.carry_egg}, Süt taşıyor: {agent.carry_milk}\")\n",
    "        print(f\"Hafızadaki tehlike sayısı: {len(agent.dangers)}\")\n",
    "    \n",
    "    return total_perf\n",
    "\n",
    "\n",
    "\n",
    "GRID, START = create_grid()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"HARITA (10x10)\")\n",
    "print(\"=\" * 50)\n",
    "for i, row in enumerate(GRID):\n",
    "    print(f\"Satır {i}: {''.join(row)}\")\n",
    "print(\"\\nLegend: E=Engel, Y=Yumurta(+2), S=Süt(+5), B=Bataklık(-10), A=Ayı(-100), C=Başlangıç\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "performances = []\n",
    "NUM_EPISODES = 5\n",
    "MAX_STEPS = 50\n",
    "\n",
    "print(f\"\\n🚜 Simülasyon başlatılıyor ({NUM_EPISODES} x {MAX_STEPS} adım)...\")\n",
    "\n",
    "# Her denemede ortamı ve ajanı sıfırla\n",
    "for i in range(NUM_EPISODES):\n",
    "    # İlk denemeyi detaylı göster\n",
    "    verbose = (i == 0)\n",
    "    if verbose:\n",
    "        print(f\"\\n DETAYLI DENEME #{i+1}:\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "   \n",
    "    perf = run_episode(GRID, START, max_steps=MAX_STEPS, verbose=verbose)\n",
    "    performances.append(perf)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n Deneme {i+1} Sonucu: {perf} puan\\n\")\n",
    "    else:\n",
    "        print(f\"Deneme {i+1}: {perf} puan\")\n",
    "\n",
    "mean_perf = statistics.mean(performances)\n",
    "max_perf = max(performances)\n",
    "min_perf = min(performances)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"📈 PERFORMANS ÖZETİ\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Performans Değerleri (Liste): {performances}\")\n",
    "print(f\"Ortalama Performans ({NUM_EPISODES} Deneme): {mean_perf:.2f} puan\")\n",
    "print(f\"En İyi Performans: {max_perf} puan\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d33bd",
   "metadata": {},
   "source": [
    "#  Agent 1 vs Agent 2 \n",
    "\n",
    "##  Genel Karşılaştırma\n",
    "\n",
    "| Özellik | **Agent 1** | **Agent 2** | **Fark / İyileşme Noktası** |\n",
    "|----------|--------------|--------------|------------------------------|\n",
    "| **Ajan Tipi** | Model-Based Reflex Agent | Model-Based Reflex Agent | Aynı temel yapı korunmuş. |\n",
    "| **Bellek Kullanımı** | Sadece gözlemlenen hücreleri kaydeder. | Hücrelerin durumuna göre ayrıca “tehlike (A, B)” etiketleri ekler. | Agent 2, **tehlike bilinci** eklemiştir. |\n",
    "| **Algı Yeteneği** | Bulunduğu hücre + 4 komşu | Bulunduğu hücre + 4 komşu | Aynı. |\n",
    "| **Karar Mekanizması** | Sadece kaynaklara yönelir veya rastgele keşif yapar. | Öncelik sırası belirlenmiştir (LOAD > Kaçınma > Hedefe ilerleme > Keşif). | Agent 2’de **öncelik tabanlı karar mantığı** eklenmiş. |\n",
    "| **Hedef Seçimi** | Manhattan mesafesine göre en yakın hedef | Ödül (puan) + mesafe birlikte değerlendirilir | Agent 2, **ödül-ağırlıklı hedef seçimi** yapıyor. |\n",
    "| **Tehlike Yönetimi** | Tehlikelerden kaçınma stratejisi yok. | Tehlikeli hücreleri hafızada işaretler ve tekrar girmekten kaçınır. | Agent 2, **aktif risk takibi** içeriyor. |\n",
    "| **Performans Ölçütü** | Kaynak ve tehlike puanları tanımlı | Aynı puan sistemi + istatistiksel analiz | Agent 2’de **epizodik test ve performans analizi** var. |\n",
    "| **Planlama / Yol Bulma** | Henüz yok (sadece Manhattan mesafesi) | Henüz yok ama geliştirme önerilerinde daha net tanımlanmış | Her iki ajan da henüz planlama yapmıyor, ancak Agent 2 bu yönde daha sistematik. |\n",
    "| **Öğrenme Yeteneği** | Henüz yok (öneri olarak belirtilmiş) | Henüz yok (öneri olarak belirtilmiş) | Benzer. |\n",
    "| **Görselleştirme / Analiz** | Yok | Öneri olarak var | Agent 2, **analiz & görselleştirme odaklı** düşünülmüş. |\n",
    "\n",
    "---\n",
    "\n",
    "##  Temel Farkların Özeti\n",
    "\n",
    "1. **Agent 2, Agent 1’in geliştirilmiş bir sürümüdür.**  \n",
    "   Agent 1 yalnızca çevreyi gözlemleyip hedefe yönelirken, Agent 2 bu bilgiyi **tehlike bilinci** ve **öncelik mantığı**yla birleştirir.\n",
    "\n",
    "2. **Karar verme süreci Agent 2’de daha katmanlıdır.**  \n",
    "   Ajan, güvenlik → hedef → keşif sıralamasında karar alır.\n",
    "\n",
    "3. **Hedef seçimi artık daha rasyonel.**  \n",
    "   Agent 2, sadece yakınlık değil, **ödül değerini** de dikkate alır.\n",
    "\n",
    "4. **Risk yönetimi eklendi.**  \n",
    "   Tehlikeli hücreler (Ayı, Bataklık) hafızada tutulur ve kaçınılır — Agent 1 bunu yapmıyordu.\n",
    "\n",
    "5. **Deneysel değerlendirme yapısı eklenmiş.**  \n",
    "   Agent 2, epizot bazlı testlerle performansını ölçer ve kıyaslar.\n",
    "\n",
    "---\n",
    "\n",
    "##  Sonuç\n",
    "\n",
    "> **Agent 2**, Agent 1’in temel prensiplerini koruyarak üzerine şu üç önemli katmanı eklemiştir:  \n",
    "> - **Risk bilinci (tehlike takibi)**  \n",
    "> - **Öncelik tabanlı karar mantığı**  \n",
    "> - **Ödül + mesafe odaklı hedef seçimi**  \n",
    "\n",
    "Bu da Agent 2’yi daha **akıllı**, **uyanık** ve **stratejik** bir ajan haline getirmiştir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f6211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deneme 1: 5 puan\n",
      "Deneme 2: 10 puan\n",
      "Deneme 3: -70 puan\n",
      "Deneme 4: -1100 puan\n",
      "Deneme 5: 0 puan\n",
      "\n",
      "Ortalama Performans: -231.00\n"
     ]
    }
   ],
   "source": [
    "#AGENT3-  Rastgele Hareketli Ajan\n",
    "\n",
    "import random\n",
    "\n",
    "class XYEnvironment:\n",
    "    def __init__(self, width=10, height=10):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grid = [[' ' for _ in range(width)] for _ in range(height)]\n",
    "        self.populate_environment()\n",
    "\n",
    "    def populate_environment(self):\n",
    "        # Rastgele konumlara yumurta, süt, bataklık ve ayı yerleştir ! \n",
    "        items = ['Y', 'S', 'B', 'A']\n",
    "        for item in items:\n",
    "            for _ in range(random.randint(3, 5)):\n",
    "                x, y = random.randint(0, self.width - 1), random.randint(0, self.height - 1)\n",
    "                self.grid[y][x] = item\n",
    "\n",
    "        # Başlangıç pozisyonu\n",
    "        self.start = (random.randint(0, self.width - 1), random.randint(0, self.height - 1))\n",
    "        self.grid[self.start[1]][self.start[0]] = 'C'\n",
    "\n",
    "    def get_cell(self, x, y):\n",
    "        if 0 <= x < self.width and 0 <= y < self.height:\n",
    "            return self.grid[y][x]\n",
    "        else:\n",
    "            return 'E'  # Engel sınır dışında\n",
    "\n",
    "    def set_cell(self, x, y, value):\n",
    "        if 0 <= x < self.width and 0 <= y < self.height:\n",
    "            self.grid[y][x] = value\n",
    "\n",
    "\n",
    "\n",
    "class Direction:\n",
    "    NORTH, EAST, SOUTH, WEST = 0, 1, 2, 3\n",
    "\n",
    "    @staticmethod\n",
    "    def turn_left(direction):\n",
    "        return (direction - 1) % 4\n",
    "\n",
    "    @staticmethod\n",
    "    def turn_right(direction):\n",
    "        return (direction + 1) % 4\n",
    "\n",
    "class FarmerAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.x, self.y = env.start\n",
    "        self.direction = random.choice([0, 1, 2, 3])\n",
    "        self.performance = 0\n",
    "        self.loaded = False\n",
    "\n",
    "    def perceive(self):\n",
    "        return self.env.get_cell(self.x, self.y)\n",
    "\n",
    "    def move_forward(self):\n",
    "        if self.direction == Direction.NORTH:\n",
    "            new_y = self.y - 1\n",
    "            new_x = self.x\n",
    "        elif self.direction == Direction.SOUTH:\n",
    "            new_y = self.y + 1\n",
    "            new_x = self.x\n",
    "        elif self.direction == Direction.EAST:\n",
    "            new_x = self.x + 1\n",
    "            new_y = self.y\n",
    "        else:\n",
    "            new_x = self.x - 1\n",
    "            new_y = self.y\n",
    "\n",
    "        if self.env.get_cell(new_x, new_y) != 'E':\n",
    "            self.x, self.y = new_x, new_y\n",
    "\n",
    "    def turn_left(self):\n",
    "        self.direction = Direction.turn_left(self.direction)\n",
    "\n",
    "    def turn_right(self):\n",
    "        self.direction = Direction.turn_right(self.direction)\n",
    "\n",
    "    def load(self):\n",
    "        cell = self.perceive()\n",
    "        if cell == 'Y':\n",
    "            self.performance += 2\n",
    "            self.env.set_cell(self.x, self.y, ' ')\n",
    "        elif cell == 'S':\n",
    "            self.performance += 5\n",
    "            self.env.set_cell(self.x, self.y, ' ')\n",
    "        self.loaded = True\n",
    "\n",
    "    def step(self):\n",
    "        # Algıla\n",
    "        cell = self.perceive()\n",
    "\n",
    "        # Ortam etkileri\n",
    "        if cell == 'B':\n",
    "            self.performance -= 10\n",
    "        elif cell == 'A':\n",
    "            self.performance -= 100\n",
    "\n",
    "        # Karar verme (basit rastgele hareket)\n",
    "        action = random.choice(['move', 'left', 'right', 'load'])\n",
    "        if action == 'move':\n",
    "            self.move_forward()\n",
    "        elif action == 'left':\n",
    "            self.turn_left()\n",
    "        elif action == 'right':\n",
    "            self.turn_right()\n",
    "        elif action == 'load':\n",
    "            self.load()\n",
    "\n",
    "    def run(self, steps=50):\n",
    "        for _ in range(steps):\n",
    "            self.step()\n",
    "        return self.performance\n",
    "\n",
    "\n",
    "\n",
    "performances = []\n",
    "\n",
    "for i in range(5):\n",
    "    env = XYEnvironment()\n",
    "    agent = FarmerAgent(env)\n",
    "    perf = agent.run(50)\n",
    "    performances.append(perf)\n",
    "    print(f\"Deneme {i+1}: {perf} puan\")\n",
    "\n",
    "ortalama = sum(performances) / len(performances)\n",
    "print(f\"\\nOrtalama Performans: {ortalama:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9431fea",
   "metadata": {},
   "source": [
    "# Agent3 - Rastgele Hareketli Model Tabanlı Refleks Ajanı  \n",
    "### (Agent1 ve Agent2 ile Karşılaştırması)\n",
    "\n",
    "##  Genel Karşılaştırma Tablosu\n",
    "\n",
    "| Özellik | **Agent1** | **Agent2** | **Agent3** | **Fark / Gelişim Düzeyi** |\n",
    "|----------|-------------|-------------|-------------|-----------------------------|\n",
    "| **Ajan Tipi** | Model-Based Reflex | Model-Based Reflex | Model-Based Reflex | Temel yapı aynı, ancak zekâ düzeyi farklı. |\n",
    "| **Karar Mekanizması** | Hedef odaklı (S, Y hücrelerine yönelir) | Hedef + risk odaklı, öncelikli karar mantığı | **Tamamen rastgele karar verir** | Agent3’te bilinçli karar mekanizması yok. |\n",
    "| **Bellek Kullanımı** | Gözlemlenen hücreleri kaydeder | Hücreleri + tehlike (A, B) bilgisiyle hafızada tutar | **Bellek yok** | Agent3, “model-based” yapıda tanımlı olsa da pratikte hafızasız çalışır. |\n",
    "| **Hedef Seçimi** | Bilinen kaynaklara yönelir | Ödül (puan) + mesafeye göre hedef seçer | **Hiç hedef belirlemez** | Agent3, tamamen rastgele hareket eder. |\n",
    "| **Tehlike Yönetimi** | Tehlikeleri fark eder ama kaçınmaz | Tehlikeleri hafızada tutar ve kaçınır | **Tehlikelerden kaçınmaz** | Agent3, bataklık ve ayıya girebilir. |\n",
    "| **Performans Ölçütü** | Kaynak toplama ve tehlike cezası | Aynı ölçüt + epizodik istatistik | **Aynı ölçüt, ama rastgelelik nedeniyle tutarsız sonuçlar** | Agent3 performansı yüksek varyanslıdır. |\n",
    "| **Yönelim Sistemi** | `Direction` enum ile yön değiştirir | Aynı | Aynı | Ortak sistem kullanılmış. |\n",
    "| **Öğrenme Yeteneği** | Henüz yok (öneri olarak belirtilmiş) | Henüz yok (öneri olarak belirtilmiş) | **Yok** | Üçü de öğrenmiyor, ancak Agent3 tamamen kör. |\n",
    "| **Planlama / Yol Bulma** | Henüz yok, öneri olarak geçiyor | Henüz yok, öneri olarak geçiyor | **Yok** | Agent3’te hiçbir planlama yok. |\n",
    "| **Strateji Türü** | Hedef odaklı refleks | Risk + hedef + öncelik odaklı refleks | **Rastgele hareket** | Agent3 saf “random exploration” uygular. |\n",
    "\n",
    "---\n",
    "\n",
    "##  Agent3’ün Temel Özellikleri\n",
    "\n",
    "1. **Rastgele Davranış**  \n",
    "   Ajan, her adımda `move`, `left`, `right` veya `load` aksiyonlarından birini tamamen rastgele seçer.  \n",
    "   Bu nedenle yönelimi veya stratejik karar mantığı yoktur.\n",
    "\n",
    "2. **Bellek veya Hedef Takibi Yok**  \n",
    "   Önceki adımlarda topladığı bilgi veya algılarını hatırlamaz.  \n",
    "   “Model-Based Reflex” olarak tanımlansa da fiilen **modeli yoktur.**\n",
    "\n",
    "3. **Performansın Değişkenliği**  \n",
    "   Rastgele kararlar nedeniyle performans büyük oranda şansa bağlıdır.  \n",
    "   Bazı epizotlarda yüksek puan toplayabilirken, bazılarında hızla puan kaybeder.\n",
    "\n",
    "4. **Basit Ama Karşılaştırma İçin Önemli**  \n",
    "   Agent3, **taban seviye kontrol ajanı** olarak kullanılabilir.  \n",
    "   Agent1 ve Agent2’nin başarımı, bu rastgele davranan ajanla karşılaştırılarak ölçülebilir.\n",
    "\n",
    "---\n",
    "\n",
    "##  Sonuç\n",
    "\n",
    "> **Agent3**, karar mekanizması veya bellek içermeyen, tamamen **rastgele hareket eden** bir ajan modelidir.  \n",
    "> Bu yapı, Agent1 ve Agent2’nin zekâ düzeyini karşılaştırmak için “kontrol grubu” görevi görebilir.\n",
    "\n",
    "**Özetle:**\n",
    "- Agent1 → Hedef odaklı refleks ajan  \n",
    "- Agent2 → Risk ve ödül bilinci olan refleks ajan  \n",
    "- Agent3 → Rastgele aksiyon alan basit ajan  \n",
    "\n",
    "Bu farklar, ajanın karar verme yeteneği arttıkça performansın **daha tutarlı ve verimli** hale geldiğini göstermektedir.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
